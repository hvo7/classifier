{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script Name: analyze_multiple16.py -- modified from make_spectrum.py\n",
    "Description:  Program to read either 1 day's data or full year, accumulate and record classification data (latitude, \n",
    "     time to Fhv transition, solar angle, FWHM, t90), write to summary fFle, and make plots \n",
    "     (time profile, energy spectra, latitude, solar activity).\n",
    "Note:  Horizontal and vertical plot limits are set (and can be adjusted) in set_plot_limit\n",
    "Version  calculates GOES avvg and variance for +/- 1 hour aroune trigger time.\n",
    "Recalculates time from hv on/off transition -- 6/17/24\n",
    "\n",
    "Usage: python ./analyze_multiple14.py [fileid(yyyymmdd) or \"Dec2023\" for example to analyze multiple days ] chan_type(PHA/PI) plot_duration (+/-, optional)\n",
    "goes\n",
    "Current version uses old version of get_phdata for fileid > 20230719 and Yuta's new version for fileid < 20230720. (5/10/2023)\n",
    "analyze_multple2 deletes individual plot files after they are merged together.\n",
    "\n",
    "Incorporates Henry's GOES solar activity data routine for +/- 1 day.\n",
    "\n",
    "Read event file and prepare list of previous classifications for each trigger number\n",
    "If len(arg[1]) != 8, then read summary file; for each line, read fileid and call analyzexx to do normal analysis\n",
    "if len(arg[1]) == 8, call analyzexx to do normal analysis only\n",
    "    For example, Dec2023 calls summary_file_Dec2023.txt to analyze data for 12/15, 12/18, 12/31/2023:\n",
    "          summary_file_20240430_2348\n",
    "          remote_file_path = /mnt/cgbmdata2/cgbm_heasarc_archive/cgbmarchL3HEA_corrected/obs/2023/20230110/auxil/cgbm_20230110.att\n",
    "          local_file_path = ./cgbml3_data/cgbm_20230110.attcgbm_20230110.att  \n",
    "          1  20231215  [1386668718, 1386688282, 1386693962, 1386698214][755948846.3019712, 755968411.1089857, 755974091.3486664, 755978343.5158461]\n",
    "          1  20231218  [1386944516, 1386950104, 1386955813][756224650.259189, 756230238.4677762, 756235946.6890068]\n",
    "          1  20231231  [1388093748, 1388094024, 1388094280][757373905.3055743, 757374181.3115886, 757374437.3316015]\n",
    "\n",
    "          Job duration = -1273.84 mins = -76431 secs.\n",
    "    Note: First 3 lines are blank, end list of data files with a blank line.\n",
    "\n",
    "For each trigger, write Prev_classification based on trigger number. (Trigger numbers may differ from list at Yoshidalab.mydns.jp by 1.)\n",
    "\n",
    "Call solar only if trig_num = 0\n",
    "Status as of 3/28/24:\n",
    "     Correctly handles multiple triggers per day\n",
    "     Needs to work on alternate plot input values, Norris backgound fitting\n",
    "Program (as of 3/21/2024): \n",
    "     Moves all Global statements to start of main routine\n",
    "     Includes t90\n",
    "     Corrects fitting parameter printouts\n",
    "     Writes name of program to output file\n",
    "     Incorporate Norris fitting function\n",
    "     determines ref_MET time\n",
    "     calculates time to closest hv turn on/off\n",
    "     plots counts vs time \n",
    "     fits background vs time \n",
    "     plots energy spectra for 6 detectors\n",
    "     Hardness ratio\n",
    "     determine latitude\n",
    "     determine angle to Sun\n",
    "     FWHM, t90 of time distribution\n",
    "     Allow for multiple triggers on a single day \n",
    "     checks for triggers on a specified day\n",
    "     chi-sq of fit\n",
    "     spectra_y_or_n removed, effectively set to \"y\"\n",
    "     Save plots and output data in summary files\n",
    "     Include likelihood calculations \n",
    "     Incorporates Henry's solar activity program.\n",
    "     Version incorporates find_triggers routiâ€ªne to find MET time.\n",
    "     This script determines time difference between trigger time and hv turn on/off.\n",
    "     Looks for hv turn-on/turn-off (HXM1L time change) > threshold counts (def = 2000) between times -2000 and 2000 sec from trigger.\n",
    "     Prints out time difference.\n",
    "     User has option to plot spectra or not\n",
    "Still:\n",
    "     Check with Yuta about hardness ratio energy ranges; need to account for background\n",
    "     Clean up redundant ouput and redundant plot1\n",
    "     Clean up outputs and pdfmerger\n",
    "     Add weighting for likelihoods\n",
    "     Add lots of comments\n",
    "     Choose L or 1-L for each parameter\n",
    "     Program frequently hangs at solar call. If I run again, it usually works fine.\n",
    "     Code only prints out counts around hv turn on/off for first trigger of the day. \n",
    "\n",
    "Author: Mike Cherry\n",
    "Date: 5/10/2024\n",
    "Version: 2\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "from astropy.table import Table, vstack\n",
    "import numpy as np\n",
    "import cgutil as cg\n",
    "import matplotlib.pyplot as plt\n",
    "#from pypdf import PdfMerger\n",
    "from pypdf import PdfWriter\n",
    "# from matplotlib.backends.backend_pdf import PdfPages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from statistics import mean\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from astropy.table import Table\n",
    "from astropy.time import Time\n",
    "from astropy.time import TimeDelta\n",
    "import astropy.units as u\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from astropy.coordinates import SkyCoord, UnitSphericalRepresentation\n",
    "from astropy.coordinates import get_sun\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input line\n",
    "args = sys.argv\n",
    "if len(args) < 3 :\n",
    "    print(\"Usage: python ./analyze_multiple2.py fileid(yyyymmdd) chan_type(PHA/PI) spectrum duration (+/-, optional)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "else:\n",
    "    fileid = str(args[1])\n",
    "    chan_type = str(args[2])\n",
    " \n",
    "if len(args) == 4:\n",
    "    tmin = -int(args[3])\n",
    "    tmax = int(args[3])\n",
    "\n",
    "else:\n",
    "    tmin= -100\n",
    "    tmax = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = '20240602 PHA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (fileid, chan_type, tmin, tmax):\n",
    "\n",
    "# Read event file and prepare list of previous classifications for each trigger number\n",
    "# Read in prior event classifications\n",
    "    global prior_trig_id, prior_trig_class\n",
    "    event_class = open (\"Event_classification.txt\", \"r\")  \t\n",
    "    lines = len(event_class.readlines())  \t\t# Determine length of file\n",
    "    event_class.seek(0)\t\t\t\t\t# Rewind to beginning\n",
    "    content = []\n",
    "    category = []  \n",
    "    prior_trig_id = []\n",
    "    prior_trig_class = []\n",
    "    for i in range (0, lines):\t\n",
    "        data = event_class.readline().strip()\t\t# Read one line at a time, strip out EOF\t\n",
    "        if (not data) or (len(data) == 0):\n",
    "            break\n",
    "        content.append (data)\t\t\t\n",
    "        category = content[i].split()     # Category = data for a single line split into individual variables\n",
    "        if category[3] == 'T':\n",
    "            prior_trig_id.append (category[1])\t\t# prior_trig_id = trigger id\n",
    "            prior_trig_class.append (category[5])\t# prior_trig_class = previously assigned event classification       \n",
    "        else:\n",
    "            prior_trig_id.append (category[1])\t\t# prior_trig_id = trigger id\n",
    "            prior_trig_class.append ('Unknown')\t\t# prior_trig_class = previously assigned event classification       \n",
    "    event_class.close()\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare looping -- one day or many\n",
    "    if len(fileid) == 8, call analyzexx to do normal analysis only\n",
    "    If len(fileid) != 8, then read run_list; for each line, read fileid and call analyzexx to do normal analysis\n",
    "    For example, Dec2023 calls run_list_Dec2023.txt to analyze data for 12/15, 12/18, 12/31/2023:\n",
    "          run_list_20240430_2348\n",
    "          remote_file_path = /mnt/cgbmdata2/cgbm_heasarc_archive/cgbmarchL3HEA_corrected/obs/2023/20230110/auxil/cgbm_20230110.att\n",
    "          local_file_path = ./cgbml3_data/cgbm_20230110.attcgbm_20230110.att  \n",
    "          1  20231215  [1386668718, 1386688282, 1386693962, 1386698214][755948846.3019712, 755968411.1089857, 755974091.3486664, 755978343.5158461]\n",
    "          1  20231218  [1386944516, 1386950104, 1386955813][756224650.259189, 756230238.4677762, 756235946.6890068]\n",
    "          1  20231231  [1388093748, 1388094024, 1388094280][757373905.3055743, 757374181.3115886, 757374437.3316015]\n",
    "\n",
    "          Job duration = -1273.84 mins = -76431 secs.\n",
    "    Note: First 3 lines are blank, end list of data files with a blank line.\n",
    "    \"\"\"\n",
    "# Analyze list of events\n",
    "# if len(fileid) == 8, call analyze39 to do normal analysis on single day only\n",
    "    if len(fileid) == 8:\t\t\n",
    "        analyze39 (fileid, chan_type, tmin, tmax)\n",
    "        sys.exit()\n",
    "\n",
    "# if len(fileid) != 8, call analyze39 to do normal analysis on all days in \"summary_file\"+fileid+\".txt\"\n",
    "    if len(fileid) != 8:\t\t\t\n",
    "#        summary_file_list = open (\"summary_file_\"+fileid+\".txt\", \"r\")\n",
    "        run_list = open (\"run_list_\"+fileid+\".txt\", \"r\")\n",
    "        content = []\n",
    "        category = []  \n",
    "        for i in range (0,3):\t\t\t\t# Skip 3 header lines\n",
    "            data = run_list.readline().strip()\t\n",
    "        i_event = 0\n",
    "        while i_event > -1:\n",
    "            data = run_list.readline().strip()\t\n",
    "            content.append (data)\t\t\t\n",
    "            category = content[i_event].split()     # Category = data for a single line split into individual variables\n",
    "            if len(category) == 0:\n",
    "                i_event = -1\n",
    "                break\n",
    "            fileid = category [1]\n",
    "            i_event += 1\n",
    "            try:\n",
    "                analyze39 (fileid, chan_type, tmin, tmax)\n",
    "            except:\n",
    "                continue\n",
    "        run_list.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def analyze39 (fileid, chan_type, tmin, tmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input example\n",
    "#fileid = '20231110'\n",
    "#chan_type = 'PHA'\n",
    "#ref_MET = 752966385.210472\n",
    "\n",
    "#    global tmino, tmax\n",
    "#    global summary_file, summary_file_name\n",
    "\n",
    "\n",
    "\n",
    "# Declare global variables\n",
    "global data_types, event_classes, max_rows, max_bins\n",
    "global xL, Ngtx\n",
    "\n",
    "# Set up local disk directory environment\n",
    "load_dotenv()\n",
    "data_dir = os.getenv(\"LOCAL_DATA_DIR\")\n",
    "\n",
    "# Set up figure saving to file\n",
    "plt.rcParams[\"figure.figsize\"] = [7.0,\n",
    "                                  0, 3.50] \n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# Account for format change in mid-2023\n",
    "if int(fileid) > 20230725:\t\t\n",
    "    ph_table = get_phdata(fileid, data_dir, chan_type)\n",
    "else:\n",
    "    ph_table = get_phdata_modified(fileid, data_dir, chan_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in likelihood tables -- new version\n",
    "\n",
    "# Declare local variables\n",
    "global data_types, event_classes, midpoints, likelihood\n",
    "\n",
    "# Input the file\n",
    "likelihood_fileid = '20240602'\t# file name for likelihood data; this can be changed if necessary\n",
    "input_file_name = \"ranking_\" + likelihood_fileid + \".txt\"\n",
    "\n",
    "# Data types\n",
    "data_types = ['Latitude','Time_to_transition','Solar_angle','Gaussian_FWHM','Gaussian_Amplitude', 'Gaussian_Chi-sq','Norris_t90',\n",
    "    'Norris_Amplitude', 'Norris_Chi-sq', 'Hardness_ratio', 'GOES', 'GOES_variance', 'GOES_avg_1hr', 'GOES_var_1hr']\t\t\t\t\n",
    "\n",
    "# Event classes\n",
    "event_classes = ['GRB', 'Solar','Particle']\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and read the data\n",
    "lhood_input = open (input_file_name, 'r')\n",
    "lines = len(lhood_input.readlines())  \t\t            # Determine length of file\n",
    "lhood_input.seek(0)                                     # Rewind to beginning\n",
    "\n",
    "lhood_input.readline()\t\t\t\t\t                # Skip past the header and go to the second line\n",
    "max_bins = int((lines - 1)/len(data_types))\t- 1\t        # Number of histogram bins must be the same for all data types\n",
    "\n",
    "likelihood_dict = {}\n",
    "\n",
    "for line in lhood_input.readlines():\n",
    "\n",
    "    line = line.strip('\\n')\n",
    "\n",
    "    print(line)\n",
    "\n",
    "    if line in data_types:               # Set a current category variable to keep track of which likelihood and midpoint values are assigned to\n",
    "        cur_category = line\n",
    "        likelihood_dict[cur_category] = []\n",
    "    else:\n",
    "        line = line.split()\n",
    "        likelihood_dict[cur_category].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Read in event list with previous characterizations.\n",
    "max_rows = 2000\n",
    "event_file_name = \"event_classification.txt\"\n",
    "#    event_id_rows, event_UT_rows, event_type_rows = []\n",
    "event_id_rows, event_UT_rows, event_type_rows = events_list (max_rows, event_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fileid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m trig_index \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m trig_time \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m triggers, trig_index, trig_time \u001b[38;5;241m=\u001b[39m determine_MET_time (\u001b[43mfileid\u001b[49m, data_dir)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#    print ('Result of finding triggers:')\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#    print (triggers)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#    print (trig_index)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#    print (trig_time)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#    num = input()   \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fileid' is not defined"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# find MET time. How about if there are multiple files on a single day?\n",
    "trig_index = []\n",
    "trig_time = []\n",
    "triggers, trig_index, trig_time = determine_MET_time (fileid, data_dir)\n",
    "#    print ('Result of finding triggers:')\n",
    "#    print (triggers)\n",
    "#    print (trig_index)\n",
    "#    print (trig_time)\n",
    "#    num = input()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Step through triggers\n",
    "for trig_num in range (len(trig_index)):\n",
    "    xyz = analyze_single_trigger (fileid, chan_type, data_dir, ph_table, trig_index, trig_time, trig_num, triggers,\n",
    "    event_id_rows, event_UT_rows, event_type_rows, tmin, tmax)\n",
    "\n",
    "return\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in likelihood tables\n",
    "def ranking (max_rows, input_file_name):\n",
    "    \"\"\"\n",
    "Description:  Program to read in, categorize likelihood tables. Based on ranking4.py.\n",
    "Program reads ranking_yyyymmdd.txt tab-delimited data file of likelihood data for \n",
    "    event classes, uses 3d list.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = [0] * len(data_types)\n",
    "\n",
    "    input_file = open(input_file_name,\"r\")\n",
    "    \n",
    "    for line in range (0,4):                     # Read and skip over header lines\n",
    "        header = input_file.readline().strip()  \n",
    "\n",
    "# Create 3d arrays\n",
    "#    global xL, Ngtx\n",
    "#    xL =   [[ ['#' for row in range(max_rows)] for col in range(len(event_classes))] for type in range(len(data_types))]\t#  x\n",
    "#    Ngtx =   [[ ['#' for row in range(max_rows)] for col in range(len(event_classes))] for type in range(len(data_types))]\t#  N(>x)\n",
    "\n",
    "    read_more_data = 'y'\n",
    "    while read_more_data == 'y':\n",
    "        content = []    \t\t\t# Content = Each line of Content is a raw data line from spreadsheet\n",
    "        category = []  \t\t\t\t# Category = data for a single line split into N(x), N(>x), etc\n",
    "        data = input_file.readline().strip()\t# Check for end of file\n",
    "        if (not data) or (len(data) == 0):\n",
    "#            print('\\nEnd of data')\n",
    "            break\n",
    "        content.append (data)\t\t\t\n",
    "        category = content[0].split()\n",
    "#        print ('\\nNew data set: ',category[0])\n",
    "        type = 'None'\n",
    "        for itype in range(len(data_types)):\t\t\n",
    "            if category[0] == data_types[itype]:\n",
    "                type = category[0]\n",
    "                break\n",
    "        if type == 'None':\n",
    "            print(\"Data_type must be one of types listed as available in code: \", data_types)\n",
    "            sys.exit(1)\n",
    "\n",
    "        header = input_file.readline().strip()\t# Read in potential event class (GRB, solar, particle) \n",
    "        content.append (header)\n",
    "        event_class = content[1].split()\n",
    "        if (event_class != event_classes):\n",
    "            print (\"Available event classes: \", event_class,'\\n',event_classes)\n",
    "            sys.exit(2)\n",
    "\n",
    "        content = []  \t\t\t\t# Read in data; first initialize content array to zero again\n",
    "        row_number = 0\t\t\t\t# Count row_number in range 0 to row_number - 1\n",
    "        while True:\t\t\t\t\t# Read in data line by line\n",
    "            data = input_file.readline().strip()\n",
    "            content.append (data)\n",
    "            category = content[row_number].split()\n",
    "            end_indicator = category[0]\n",
    "            if (end_indicator == \"***\"):\n",
    "                break \n",
    "\n",
    "            for j in range(len(event_classes)):\t\t\t\t\t# GRB, solar, or particle ?\n",
    "                xL [itype][j][row_number] = eval (category [4 + j])\n",
    "#                Ngtx [itype][j][row_number] = category[7 + j]    \t\t# Use integral distribution\n",
    "                Ngtx [itype][j][row_number] = eval (category[13 + j])\t\t# Use differentiated integral distribution\n",
    "\n",
    "            row_number += 1\t\t\t\t# row_number is now actual count of rows that have been read in\n",
    "            rows [itype] = row_number\n",
    "\n",
    "            if row_number > max_rows:\n",
    "                print ('Maximum number of rows (',max_rows,' has been exceeded. max_rows must be increased.')\n",
    "                sys.exit()\n",
    "        continue\n",
    "    input_file.close()\n",
    " \n",
    "    return rows\n",
    "\n",
    "###############################################################  \n",
    "def events_list (max_rows, event_file_name):\n",
    "    event_file = open(event_file_name,\"r\")\n",
    "\n",
    "    event_id_rows = []\n",
    "    event_UT_rows = []\n",
    "    event_type_rows = []\n",
    "    content = []    \t\t\t# Content = Each line of Content is a raw data line from spreadsheet\n",
    "    category = []  \t\t\t\t# Category = data for a single line split into N(x), N(>x), etc\n",
    "    for ievent_num in range (max_rows):\n",
    "        data = event_file.readline().strip()\t# Check for end of file\n",
    "        if (not data) or (len(data) == 0):\n",
    "            break\t\t\n",
    "        category = data.split()\n",
    "        if len(category) < 6:\n",
    "            data = event_file.readline().strip()\t# Check for end of file\n",
    "            if (not data) or (len(data) == 0):\n",
    "                break\t\t\n",
    "            category = data.split()\n",
    "        event_id_rows.append(category[1])\n",
    "        event_UT_rows.append(category[2])\n",
    "        if category[5] == 'LGRB' or category[5] == 'SGRB' or category[5] == 'PGRB':\n",
    "            category[5] = event_classes[0]\t\t\t# GRB\n",
    "        if category[5] == 'SFLR':\t\t\t\t# solar flare\n",
    "            category[5] = event_classes[1]\n",
    "        if category[5] == 'PART':\t\t\t\t# Particle\n",
    "            category[5] = event_classes[2]\n",
    "        event_type_rows.append(category[5])\n",
    "#        print (event_id_rows[ievent_num], event_UT_rows[ievent_num], event_type_rows[ievent_num])  \n",
    "    event_file.close()\t\t\t\t\t\t\n",
    "#    print ('Length:  ',len(event_id_rows))\n",
    "    return event_id_rows, event_UT_rows, event_type_rows\n",
    "\n",
    "###############################################################\n",
    "def determine_MET_time (fileid,data_dir):\n",
    "#     Returns triggers, trig_index,trig_time\n",
    "\n",
    "    hk_file = '%s/cgbm_%s.hk' % (data_dir, fileid)\n",
    "    hk_table = Table.read (hk_file, hdu=1)\n",
    "\n",
    "    # Timestamps of CGBM data are end of the bins.\n",
    "    # Timestamps were changed to center of the bins\n",
    "    hk_table['TIME'] = hk_table['TIME'] - cg.PERIODIC_HBIN\n",
    "    hk_table['MDCTIME'] = hk_table['MDCTIME'] - cg.PERIODIC_HBIN\n",
    "\n",
    "    # The transition of TRIG_STATUS (from 0 to 1) means onboard trigger happened.\n",
    "    hk_table['TRIG_TRANSITION'] = 0\n",
    "    hk_table['TRIG_TRANSITION'][0] = 0\n",
    "    hk_table['TRIG_TRANSITION'][1:] = hk_table['TRIG_STATUS'].data[1:] - hk_table['TRIG_STATUS'].data[:-1]\n",
    "\n",
    "    # I don't know why this is needed but TRIG_TRANSITION can be unexpected values.\n",
    "    bad_mask = (hk_table['TRIG_TRANSITION'] == 0) | (hk_table['TRIG_TRANSITION'] == 1)\n",
    "    hk_table['TRIG_TRANSITION'][~bad_mask] = 0\n",
    "\n",
    "    trig_index = hk_table['TRIG_TRANSITION'] == 1\n",
    "\n",
    "    # Extracting rows of which TRIG_TRANSITION == 1\n",
    "    triggers = hk_table[trig_index]\n",
    "\n",
    "    # Counting and Printing the number of triggers\n",
    "    print('\\n%d triggers in %s ' % (len(triggers), fileid))\n",
    "\n",
    "    trig_index = []\n",
    "    trig_time = []\n",
    "    # Printing the TriggerID and Trigger time\n",
    "    indexp = 0\n",
    "    for row in triggers:\n",
    "        indexp +=1\n",
    "        print('TriggerID = %d   Trigger Time (MET) = %.6f   Trigger Time (MDC) = %.6f' \n",
    "            % (int(row['MDCTIME']), float(row['TIME']), float(row['MDCTIME'])))\n",
    "        trig_index.append (int(row['MDCTIME']))\n",
    "        trig_time.append (float(row['TIME']))\n",
    "    print ()\n",
    "\n",
    "    return triggers, trig_index,trig_time\n",
    "\n",
    "###############################################################\n",
    "def get_phdata(fileid, data_dir, chan_type):\n",
    "#  Returns merged\n",
    "    if not (chan_type == 'PHA' or chan_type == 'PI'):\n",
    "        print('chan_type is not expected.')\n",
    "        return None\n",
    "\n",
    "    hxm1h_ph_file = '%s/cgbm_%s_hx1_hph.fits' % (data_dir, fileid)\n",
    "    hxm1h_ph_table = Table.read(hxm1h_ph_file, hdu=1)\n",
    "    hxm1l_ph_file = '%s/cgbm_%s_hx1_lph.fits' % (data_dir, fileid)\n",
    "    hxm1l_ph_table = Table.read(hxm1l_ph_file, hdu=1)\n",
    "\n",
    "    hxm2h_ph_file = '%s/cgbm_%s_hx2_hph.fits' % (data_dir, fileid)\n",
    "    hxm2h_ph_table = Table.read(hxm2h_ph_file, hdu=1)\n",
    "    hxm2l_ph_file = '%s/cgbm_%s_hx2_lph.fits' % (data_dir, fileid)\n",
    "    hxm2l_ph_table = Table.read(hxm2l_ph_file, hdu=1)\n",
    "\n",
    "    sgmh_ph_file = '%s/cgbm_%s_sgm_hph.fits' % (data_dir, fileid)\n",
    "    sgmh_ph_table = Table.read(sgmh_ph_file, hdu=1)\n",
    "    sgml_ph_file = '%s/cgbm_%s_sgm_lph.fits' % (data_dir, fileid)\n",
    "    sgml_ph_table = Table.read(sgml_ph_file, hdu=1)\n",
    "\n",
    "    # 'TIME' is Misson Elapsed Time (MET) same as Suzaku/MAXI\n",
    "    # MET = 0 is 2000-01-01T00:00:00 UTC\n",
    "    # 'MDCTIME' is time in MDC time\n",
    "    merged = hxm1h_ph_table[['TIME', 'MDCTIME']]   \n",
    "    merged = hxm1l_ph_table[['TIME', 'MDCTIME']]\n",
    "\n",
    "    # '*_EXPOSURE's are exposure for each detector.\n",
    "    merged['HXM1_EXPOSURE'] = hxm1h_ph_table['EXPOSURE']\n",
    "    merged['HXM1_EXPOSURE'] = hxm1l_ph_table['EXPOSURE']\n",
    "\n",
    "    merged['HXM2_EXPOSURE'] = hxm2h_ph_table['EXPOSURE']\n",
    "    merged['SGM_EXPOSURE'] = sgmh_ph_table['EXPOSURE']\n",
    "#    print (sgmh_ph_table['PHA'])\n",
    "#    merged\n",
    "#    print (merged['SGM_EXPOSURE'])\n",
    "#    input()\n",
    "\n",
    "    # '*_COUNTS's are counts per PH time binning (4s)\n",
    "    merged['HXM1H_COUNTS'] = hxm1h_ph_table[chan_type]\n",
    "    merged['HXM1L_COUNTS'] = hxm1l_ph_table[chan_type]\n",
    "    merged['HXM2H_COUNTS'] = hxm2h_ph_table[chan_type]\n",
    "    merged['HXM2L_COUNTS'] = hxm2l_ph_table[chan_type]\n",
    "    merged['SGMH_COUNTS'] = sgmh_ph_table[chan_type]\n",
    "    merged['SGML_COUNTS'] = sgml_ph_table[chan_type]\n",
    "\n",
    "    merged['HXM1H_RATE'] = merged['HXM1H_COUNTS'] / np.tile(merged['HXM1_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['HXM2H_RATE'] = merged['HXM2H_COUNTS'] / np.tile(merged['HXM2_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['SGMH_RATE'] = merged['SGMH_COUNTS'] / np.tile(merged['SGM_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['HXM1L_RATE'] = merged['HXM1L_COUNTS'] / np.tile(merged['HXM1_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "    merged['HXM2L_RATE'] = merged['HXM2L_COUNTS'] / np.tile(merged['HXM2_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "    merged['SGML_RATE'] = merged['SGML_COUNTS'] / np.tile(merged['SGM_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "\n",
    "    return merged\n",
    "###############################################################\n",
    "\n",
    "def get_phdata_modified(fileid, data_dir, chan_type):\n",
    "    if not (chan_type == 'PHA' or chan_type == 'PI'):\n",
    "        print('chan_type is not expected.')\n",
    "        return None\n",
    "\n",
    "    hxm1h_ph_file = '%s/cgbm_%s_hx1_hph.fits' % (data_dir, fileid)\n",
    "    hxm1h_ph_table = Table.read(hxm1h_ph_file, hdu=1)\n",
    "    hxm1l_ph_file = '%s/cgbm_%s_hx1_lph.fits' % (data_dir, fileid)\n",
    "    hxm1l_ph_table = Table.read(hxm1l_ph_file, hdu=1)\n",
    "\n",
    "    hxm2h_ph_file = '%s/cgbm_%s_hx2_hph.fits' % (data_dir, fileid)\n",
    "    hxm2h_ph_table = Table.read(hxm2h_ph_file, hdu=1)\n",
    "    hxm2l_ph_file = '%s/cgbm_%s_hx2_lph.fits' % (data_dir, fileid)\n",
    "    hxm2l_ph_table = Table.read(hxm2l_ph_file, hdu=1)\n",
    "\n",
    "    sgmh_ph_file = '%s/cgbm_%s_sgm_hph.fits' % (data_dir, fileid)\n",
    "    sgmh_ph_table = Table.read(sgmh_ph_file, hdu=1)\n",
    "    sgml_ph_file = '%s/cgbm_%s_sgm_lph.fits' % (data_dir, fileid)\n",
    "    sgml_ph_table = Table.read(sgml_ph_file, hdu=1)\n",
    "\n",
    "    # 'TIME' is Misson Elapsed Time (MET) same as Suzaku/MAXI\n",
    "    # MET = 0 is 2000-01-01T00:00:00 UTC\n",
    "    # 'MDCTIME' is time in MDC time\n",
    "    merged = hxm1h_ph_table[['TIME', 'MDCTIME']]\n",
    "\n",
    "    # '*_EXPOSURE's are exposure for each detector.\n",
    "    merged['HXM1_EXPOSURE'] = hxm1h_ph_table['EXPOSURE']\n",
    "    merged['HXM2_EXPOSURE'] = hxm2h_ph_table['EXPOSURE']\n",
    "    merged['SGM_EXPOSURE'] = sgmh_ph_table['EXPOSURE']\n",
    "\n",
    "    # '*_COUNTS's are counts per PH time binning (4s)\n",
    "    chan_type = 'COUNTS_' + chan_type\n",
    "    merged['HXM1H_COUNTS'] = hxm1h_ph_table[chan_type]\n",
    "    merged['HXM1L_COUNTS'] = hxm1l_ph_table[chan_type]\n",
    "    merged['HXM2H_COUNTS'] = hxm2h_ph_table[chan_type]\n",
    "    merged['HXM2L_COUNTS'] = hxm2l_ph_table[chan_type]\n",
    "    merged['SGMH_COUNTS'] = sgmh_ph_table[chan_type]\n",
    "    merged['SGML_COUNTS'] = sgml_ph_table[chan_type]\n",
    "\n",
    "    merged['HXM1H_RATE'] = merged['HXM1H_COUNTS'] / np.tile(merged['HXM1_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['HXM2H_RATE'] = merged['HXM2H_COUNTS'] / np.tile(merged['HXM2_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['SGMH_RATE'] = merged['SGMH_COUNTS'] / np.tile(merged['SGM_EXPOSURE'], (cg.PH_BINNUM_HIGH, 1)).T\n",
    "    merged['HXM1L_RATE'] = merged['HXM1L_COUNTS'] / np.tile(merged['HXM1_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "    merged['HXM2L_RATE'] = merged['HXM2L_COUNTS'] / np.tile(merged['HXM2_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "    merged['SGML_RATE'] = merged['SGML_COUNTS'] / np.tile(merged['SGM_EXPOSURE'], (cg.PH_BINNUM_LOW, 1)).T\n",
    "\n",
    "    return merged\n",
    "\n",
    "###############################################################\n",
    "def set_plot_limits (x, yarray):\n",
    "    xmin = -100\n",
    "    xmax = 100\n",
    "    ymin = 1000000000\n",
    "    ymax = 0\n",
    "    for i in range (len(x)):\n",
    "        if x [i] >= xmin :\n",
    "            if x[i] > xmax:\n",
    "                break\n",
    "            yi = np.sum(yarray, axis = 1)[i]\n",
    "            if yi < ymin:\n",
    "                ymin = yi\n",
    "            if yi > ymax:\n",
    "                ymax = yi\n",
    "    ymin = int(ymin/500)*500\n",
    "    ymax = (int(ymax/500) + 1) * 500\n",
    "    return xmin, xmax, ymin, ymax\n",
    "\n",
    "###############################################################\n",
    "# Routine to plot counts vs time     \n",
    "def time_dist(x, y_HXM1L, y_HXM2L, y_SGML, ref_MET, fileid, trig_num):\n",
    "    fig, cv = plt.subplots(3, 1, sharex=True, figsize=(8, 6))\n",
    "    #    cv[0].set_xlim(-1800, 1800)\n",
    "    xmin, xmax, ymin, ymax = set_plot_limits(x, y_HXM1L)\n",
    "    cv[0].set_xlim(xmin,xmax)\n",
    "    cv[0].set_ylim(ymin,ymax)\n",
    "    cv[0].plot(x, np.sum(y_HXM1L, axis=1), drawstyle='steps-mid', label='HXM1')\n",
    "    xmin, xmax, ymin, ymax = set_plot_limits(x, y_HXM2L)\n",
    "    cv[1].set_ylim(ymin,ymax)\n",
    "    cv[1].plot(x, np.sum(y_HXM2L, axis=1), drawstyle='steps-mid', label='HXM2')\n",
    "    xmin, xmax, ymin, ymax = set_plot_limits(x, y_SGML)\n",
    "    cv[2].set_ylim(ymin,ymax)\n",
    "    cv[2].plot(x, np.sum(y_SGML, axis=1), drawstyle='steps-mid', label='SGM')\n",
    " \n",
    "    id = 'Trigger #' + fileid + '.' + str(trig_num)\n",
    "    cv[2].set_xlabel('Time [s] since %.6f -- %s' % (ref_MET, id), fontsize=14)\n",
    "    plt.subplots_adjust(hspace=0, top=0.95, bottom=0.1, left=0.15, right=0.95)\n",
    "\n",
    "    for ax in cv:\n",
    "        ax.axvline(x=0, color='red', linestyle='--')\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_ylabel('Counts/s', fontsize=14)\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "#    print (' \\nClose plot to continue.')\n",
    "#    plt.savefig ('Plot1.pdf')\n",
    "#    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "###############################################################\n",
    "def fitter(x_data, y_data, ref_MET, tmin, tmax):\n",
    "# Version of  .                   for Gaussian function\n",
    "#   Returns ampl, peak_pos, bkg, s_over_rootB, FWHM, chisq\n",
    "\n",
    "    average = mean(np.sum(y_data, axis = 1))\n",
    "    peak = max(np.sum(y_data, axis = 1))\n",
    "\n",
    "# generate data over restricted range tmin to tmax defined as global variables at start of main routine\n",
    "    x_data0 = []\t\t# x_data, y_data are original data\n",
    "    y_data0 = []\t\t# x_data0, y_data0 are data defined in restricted range\n",
    "\n",
    "    ij = 0\n",
    "    for i in range (0,len(x_data)):\n",
    "        if ((x_data[i] >= tmin) & (x_data[i] <= tmax)):\n",
    "            x_data0.append (x_data[i])\n",
    "            y_data0.append (np.sum(y_data, axis = 1)[i])\n",
    "            ij += 1\n",
    "#            print ('Gauss:  ', xdata0[i]. ydata0[i])\n",
    "\n",
    "    params = [0.,0.,20., 2., .03, .0003]\t\t# Fitting parameters\n",
    "    a = ampl = peak - average \n",
    "    b = peak_pos = params[1] \n",
    "    c = FWHM = params[2] \n",
    "    d = bkg1 = average \n",
    "    e = bkg_linear = params[4]\n",
    "    f = bkg_quad = params[5]\n",
    "\n",
    "    y_fit = fit_function (x_data0, a, b, c, d, e, f)  \n",
    "\n",
    "# curve_fit() function takes the test-function x-data0 and y-data0 as arguments and returns \n",
    "# the coefficients a - f in param and the estimated covariance of param in param_cov\n",
    " \n",
    "    p0 = [ a, b, c, d, e, f ]\t\t\t\t# Initial values of fitting parameters\n",
    "    try:\n",
    "        param, param_cov = curve_fit(fit_function, x_data0, y_data0, p0, sigma=None, absolute_sigma=True,maxfev = 100000)\n",
    "    except:\n",
    "        print ('No Gaussian fit possible (maxfev > 10000).')\n",
    "        param = [-1, -1, -1, -1, -1, -1]\n",
    "# Calculate chi-squared\n",
    "    chisq = 0\n",
    "    for i in range (0,len(x_data0)):   \n",
    "        y_fit[i] = fit_function (x_data0[i], *param)\n",
    "        chisq += (y_fit[i] - y_data0[i]) ** 2\n",
    "    chisq = chisq / (len(x_data0)-1)\n",
    "\n",
    "#     print(\"Fitting function coefficients:\", param, \"     Chi-squared: \", chisq)\n",
    "#    print(\"Covariance of coefficients:\")\n",
    "#    print(param_cov)\n",
    "\n",
    "    plt.plot(x_data0, y_data0, 'o', color ='red', label =\"data\")\n",
    "    FWHM = 2.355 * round(param[2],1)\n",
    "    ampl = param [0]\n",
    "    peak_pos = param [1]\n",
    "    bkg = d + e * peak_pos + f * peak_pos * peak_pos\n",
    "    s_over_rootB = ampl/math.sqrt(bkg)\n",
    "    FWHM = abs(round (FWHM, 1))\n",
    "\n",
    "#Plot fitted function\n",
    "    plt.plot(x_data0, fit_function(x_data0, *param), '--', color ='blue', label =f\"Gaussian function:  FWHM = {FWHM} s\")\n",
    "    plt.legend()\n",
    "#    plt.savefig ('Plot_Gaussian.pdf')\n",
    "#    plt.show()\n",
    "\n",
    "    return ampl, peak_pos, bkg, s_over_rootB, FWHM, chisq, d, e, f\n",
    "\n",
    "###############################################################\n",
    "# Gaussian fit function with quadratic background\n",
    "def fit_function (x, a, b, c, d, e, f):\n",
    "    return a * np.exp(-.5*((np.array(x)-b)/c)**2) + d + np.array(x) * e + np.square(x) * f \n",
    "\n",
    "###############################################################\n",
    "def fitter_Norris(x_data, y_data, ref_MET, peak_pos1, FWHM, d0, e0, f0):\n",
    "# Version of fitter for Norris function\n",
    "#   Returns ampl, peak_pos, bkg, s_over_rootB, t90, chisq\n",
    "\n",
    "    tmin0 = -4 * abs(FWHM)            # Perform Norris fit only over restricted range around Gaussian peak\n",
    "    tmax0 = 4 * abs(FWHM)\n",
    "\n",
    "# generate data over restricted range tmin0 to tmax0\n",
    "    x_data0 = []\n",
    "    y_data0 = []\n",
    "\n",
    "    ij = 0\n",
    "    peak_pos = peak_pos1\n",
    "    peak = 0\n",
    "    for i in range (0,len(x_data)):\n",
    "        if x_data[i] > tmax0:\n",
    "            break\n",
    "        if x_data[i] >= tmin0 :\n",
    "            x_data0.append (x_data[i])\n",
    "            y_data0.append (np.sum(y_data, axis = 1)[i])\n",
    "            bkgi = d0 + e0 *  x_data[i] + f0 * x_data[i] * x_data[i] \n",
    "            y_data0[ij] = y_data0[ij] - bkgi\t\t# Subtract off background estimated for Gaussian\n",
    "            if (abs(x_data[i]) - peak_pos) < 4 :\n",
    "                bkg = bkgi\n",
    "                peak = y_data0[ij]\n",
    "            ij += 1\n",
    "\n",
    "    tau1 = tau2 = FWHM  \n",
    "    a = peak * np.exp(2)\t\t\t# Differentiate Norris function, set to zero to determine amplitude\n",
    "    b = peak_pos - FWHM   \t\t\t# Initial estimate of starting point\n",
    "\n",
    "#    params = [a, b, tau1, tau2] \n",
    "\n",
    "# curve_fit() function takes the test-function x-data and y-data as argument and returns \n",
    "# the coefficients a, b, tau1, tau2 in param and the estimated covariance of param in param_cov\n",
    " \n",
    "    p0 = [ a, b, tau1, tau2, d0, e0, f0 ]\t# Initial values of fit parameters\n",
    "    try:\n",
    "        param, param_cov = curve_fit(fit_Norris, x_data0, y_data0, p0, maxfev = 10000)\n",
    "    except:\n",
    "        print ('No Norris fit possible (maxfev > 10000).')    \n",
    "        param = [-1,-1,-1,-1,-1,-1,-1]\n",
    "    peak = param[0]\n",
    "    b = param[1]\n",
    "    param[2] = abs(param[2])\n",
    "    param[3] = abs(param[3])\n",
    "    tau1 = param[2]\n",
    "    tau2 = param[3]\n",
    "    e = param[4]\n",
    "    f = param[5]\n",
    "    g = param[6]\n",
    "\n",
    "    y_Norris = fit_Norris (x_data0, peak, b, tau1, tau2, e, f, g)\n",
    "\n",
    "# Calculate t90 from fit\n",
    "    sumy1 = sumy = 0\n",
    "    t0 = b\n",
    "    t90 = 1000\n",
    "    for i in range (0,len(x_data0)): \n",
    "        bkgi = e + f * x_data0[i] + g * x_data0[i] * x_data0[i]\n",
    "        sumy += y_Norris[i] - bkgi\t\t\t# Remove background, calculate total number of counts\n",
    "    for i in range (0,len(x_data0)): \n",
    "        bkgi = e + f * x_data0[i] + g * x_data0[i] * x_data0[i]\n",
    "        sumy1 += y_Norris[i] - bkgi\n",
    "        if (sumy1 >= .05 * sumy) and (t0 == b) :\n",
    "            t0 = x_data0[i]\n",
    "        if sumy1 >= .95 * sumy:\n",
    "            t90 = round (x_data0[i] - t0, 1)\n",
    "            break\n",
    "# Calculate chi-squared, maxy, peak_pos, bkg2\n",
    "    chisq = 0   \n",
    "    for i in range (0,len(x_data0)):   \t\n",
    "        bkgi = d0 + e0 * x_data0[i] + f0 * x_data0[i] * x_data0[i]\n",
    "        y_Norris[i] += bkgi\t\t\t# Add the Gaussian background back in \n",
    "        y_data0[i] += bkgi\n",
    "        chisq += (y_Norris[i] - y_data0[i]) ** 2\n",
    "#        print ('Norris   :', xdata0[i], ydata0[i])\n",
    "    chisq = chisq / (len(x_data0)-1)\n",
    "\n",
    "    maxy = peak * np.exp(-(2 * math.sqrt(tau2/tau1)))\n",
    "    peak_pos = b + math.sqrt(tau1*tau2)\n",
    "    bkg = e + d0 + (f + e0) * peak_pos + (g + f0) * peak_pos * peak_pos\n",
    "    s_over_rootB = maxy/math.sqrt(max(bkg,0.01))\n",
    "    plt.plot(x_data0, y_Norris, '+', color ='green', label =f\"Norris function:  t90 = {t90} s\")\n",
    "    plt.legend(fontsize = '6',loc='upper left')\n",
    "    plt.savefig ('Plot2.pdf')\n",
    "#    plt.show()\n",
    "#    plt.close()\n",
    "\n",
    "    return maxy, peak_pos, bkg, s_over_rootB, t90, chisq\n",
    "\n",
    "###############################################################\n",
    "# Norris et al 2005 fit function with quadratic background\n",
    "def fit_Norris (x, a, b, c, d, e, f, g):\n",
    "\n",
    "    yNorris = e + np.array(x) * f + np.square(x) * g\n",
    "    for i in range (0, len(x)):\n",
    "        if x[i] > 1000:\n",
    "            break\n",
    "        if x[i] > b :\n",
    "            yNorris[i] += a * np.exp(-(x[i]-b)/c - d/(x[i]-b))    \n",
    "\n",
    "    return yNorris\n",
    "\n",
    "#   return a * np.exp(-(np.array(x)-b)/c - d/(np.array(x)-b))  + e + np.array(x) * f + np.square(x) * g \n",
    "\n",
    "###############################################################\n",
    "def get_th_data(data_dir, fileid):\n",
    "    \"\"\"\n",
    "        This function loads six TH data files and merge them to one astropy Table.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): path to the data directory\n",
    "            fileid (str): date of the data (yyyymmdd)\n",
    "\n",
    "        Returns:\n",
    "            Table: a table includes six TH data for each detector & gain\n",
    "    \"\"\"\n",
    "\n",
    "    hx1_hth_file = '%s/cgbm_%s_hx1_hth.fits' % (data_dir, fileid)\n",
    "    hx1_hth_table = Table.read(hx1_hth_file, hdu=1)\n",
    "    hx2_hth_file = '%s/cgbm_%s_hx2_hth.fits' % (data_dir, fileid)\n",
    "    hx2_hth_table = Table.read(hx2_hth_file, hdu=1)\n",
    "    sgm_hth_file = '%s/cgbm_%s_sgm_hth.fits' % (data_dir, fileid)\n",
    "    sgm_hth_table = Table.read(sgm_hth_file, hdu=1)\n",
    "    hx1_lth_file = '%s/cgbm_%s_hx1_lth.fits' % (data_dir, fileid)\n",
    "    hx1_lth_table = Table.read(hx1_lth_file, hdu=1)\n",
    "    hx2_lth_file = '%s/cgbm_%s_hx2_lth.fits' % (data_dir, fileid)\n",
    "    hx2_lth_table = Table.read(hx2_lth_file, hdu=1)\n",
    "    sgm_lth_file = '%s/cgbm_%s_sgm_lth.fits' % (data_dir, fileid)\n",
    "    sgm_lth_table = Table.read(sgm_lth_file, hdu=1)\n",
    "    merge = Table()\n",
    "\n",
    "    # Timestamps of CGBM data are end of the bins.\n",
    "    # Timestamps were changed to center of the bins\n",
    "    merge['TIME'] = hx1_hth_table['TIME'] - cg.TH_HBIN\n",
    "    merge['MDCTIME'] = hx1_hth_table['MDCTIME'] - cg.TH_HBIN\n",
    "    merge['HXM1_EXPOSURE'] = hx1_hth_table['EXPOSURE']\n",
    "    merge['HXM1H_COUNTS'] = hx1_hth_table['COUNTS']\n",
    "    merge['HXM1H_RATE'] = (\n",
    "            hx1_hth_table['COUNTS']\n",
    "            / np.array([\n",
    "        hx1_hth_table['EXPOSURE'],\n",
    "        hx1_hth_table['EXPOSURE'],\n",
    "        hx1_hth_table['EXPOSURE'],\n",
    "        hx1_hth_table['EXPOSURE']]).T)\n",
    "    merge['HXM1L_COUNTS'] = hx1_lth_table['COUNTS']\n",
    "    merge['HXM1L_RATE'] = (\n",
    "            hx1_lth_table['COUNTS']\n",
    "            / np.array([\n",
    "        hx1_lth_table['EXPOSURE'],\n",
    "        hx1_lth_table['EXPOSURE'],\n",
    "        hx1_lth_table['EXPOSURE'],\n",
    "        hx1_lth_table['EXPOSURE']]).T)\n",
    "\n",
    "    merge['HXM2_EXPOSURE'] = hx2_hth_table['EXPOSURE']\n",
    "    merge['HXM2H_COUNTS'] = hx2_hth_table['COUNTS']\n",
    "    merge['HXM2H_RATE'] = (\n",
    "            hx2_hth_table['COUNTS']\n",
    "            / np.array([\n",
    "        hx2_hth_table['EXPOSURE'],\n",
    "        hx2_hth_table['EXPOSURE'],\n",
    "        hx2_hth_table['EXPOSURE'],\n",
    "        hx2_hth_table['EXPOSURE']]).T)\n",
    "    merge['HXM2L_COUNTS'] = hx2_lth_table['COUNTS']\n",
    "    merge['HXM2L_RATE'] = (\n",
    "            hx1_lth_table['COUNTS']\n",
    "            / np.array([\n",
    "        hx2_lth_table['EXPOSURE'],\n",
    "        hx2_lth_table['EXPOSURE'],\n",
    "        hx2_lth_table['EXPOSURE'],\n",
    "        hx2_lth_table['EXPOSURE']]).T)\n",
    "\n",
    "    merge['SGM_EXPOSURE'] = sgm_hth_table['EXPOSURE']\n",
    "    merge['SGMH_COUNTS'] = sgm_hth_table['COUNTS']\n",
    "    merge['SGMH_RATE'] = (\n",
    "            sgm_hth_table['COUNTS']\n",
    "            / np.array([\n",
    "        sgm_hth_table['EXPOSURE'],\n",
    "        sgm_hth_table['EXPOSURE'],\n",
    "        sgm_hth_table['EXPOSURE'],\n",
    "        sgm_hth_table['EXPOSURE']]).T)\n",
    "    merge['SGML_COUNTS'] = sgm_lth_table['COUNTS']\n",
    "    merge['SGML_RATE'] = (\n",
    "            sgm_lth_table['COUNTS']\n",
    "            / np.array([\n",
    "        sgm_lth_table['EXPOSURE'],\n",
    "        sgm_lth_table['EXPOSURE'],\n",
    "        sgm_lth_table['EXPOSURE'],\n",
    "        sgm_lth_table['EXPOSURE']]).T)\n",
    "    return merge\n",
    "\n",
    "###############################################################\n",
    "def plot_orb(data_dir, fileid, trigtime):\n",
    "    \"\"\"\n",
    "        Routine copied from process_trigger to make an orbit plot and save as png file.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): path to the data directory\n",
    "            fileid (str): date of the data (yyyymmdd)\n",
    "            trigtime (float): trigger time in MET\n",
    "        Returns:\n",
    "            orb_table['LATITUDE'][diff_min_index], orb_table['LONGITUDE'][diff_min_index]\n",
    "\n",
    "    \"\"\"\n",
    "    # mergin is a variable to set the time interval for the track of the pointing direction.\n",
    "    mergin = 150.0\n",
    "    orb_file = '%s/cgbm_%s.orb' % (data_dir, fileid)\n",
    "    orb_table = Table.read(orb_file, hdu=1)\n",
    "    orb_table['TIME'] = orb_table['TIME'] - cg.PERIODIC_HBIN\n",
    "    neg_mask = orb_table['LONGITUDE'] > 180\n",
    "    orb_table['LONGITUDE'][neg_mask] = orb_table['LONGITUDE'][neg_mask] - 360\n",
    "\n",
    "    # Find the nearest time stamp\n",
    "    orb_table['abs_diff'] = np.abs(orb_table['TIME'] - trigtime)\n",
    "    diff_min_index = np.argmin(orb_table['abs_diff'])\n",
    "\n",
    "    # Extracting row within triggertime +/- mergin\n",
    "    around_mask = (trigtime - mergin <= orb_table['TIME']) & ( orb_table['TIME'] <= trigtime + mergin)\n",
    "    around = orb_table[around_mask]\n",
    "\n",
    "#    print ('Latitude = ',orb_table['LATITUDE'][diff_min_index],'   Longitude = ', orb_table['LONGITUDE'][diff_min_index])\n",
    "    \n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.set_global()\n",
    "    ax.coastlines(resolution='110m', linewidth=1, edgecolor='black')\n",
    "    plt.plot(\n",
    "        around['LONGITUDE'],\n",
    "        around['LATITUDE'],\n",
    "        marker=',',\n",
    "        ls='None')\n",
    "\n",
    "    plt.plot(\n",
    "        orb_table['LONGITUDE'][diff_min_index],\n",
    "        orb_table['LATITUDE'][diff_min_index],\n",
    "        color='purple',\n",
    "        marker='*',\n",
    "        ls='None'\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(range(-180, 210, 30), crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(range(-90, 120, 30), crs=ccrs.PlateCarree())\n",
    "    ax.xaxis.set_major_formatter(plt.FixedFormatter(ax.get_xticks()))\n",
    "    ax.yaxis.set_major_formatter(plt.FixedFormatter(ax.get_yticks()))\n",
    "    plt.xlabel('Longitude [deg]')\n",
    "    plt.ylabel('Latitude [deg]')\n",
    "    plt.savefig ('Plot4.pdf')\n",
    "#    plt.show()\n",
    "    plt.close ('all')\n",
    "    return orb_table['LATITUDE'][diff_min_index], orb_table['LONGITUDE'][diff_min_index]\n",
    "\n",
    "###############################################################\n",
    "def met2utc(trigtime_met):\n",
    "    \"\"\"\n",
    "        This function get Time in astropy.time from trigger time in MET\n",
    "\n",
    "        Args:\n",
    "            trigtime_met (float): trigger time in MET\n",
    "        Returns:\n",
    "            utc (astropy.time.Time):\n",
    "    \"\"\"\n",
    "    met = TimeDelta(trigtime_met, format=\"sec\")\n",
    "    utc = (met + cg.T0)\n",
    "    return utc\n",
    "\n",
    "###############################################################\n",
    "def get_iss_att(trigtime_met, iat_table):\n",
    "    \"\"\"\n",
    "        This function get ISS attitude data.\n",
    "\n",
    "        Args:\n",
    "            trigtime_met (float): trigger time in MET\n",
    "            iat_table (astropy.table.Table): Table of orbit data\n",
    "        Returns:\n",
    "            iss2sky (scipy.spatial.transform.Rotation): Rotation matrix from ISS\n",
    "            coordinate to the celestial coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    iat_table['ZTIME'] = iat_table['TIME'] - trigtime_met\n",
    "\n",
    "    # Find the nearest time stamp\n",
    "    iat_table['ABS_TIME'] = np.abs(iat_table['TIME'] - trigtime_met)\n",
    "    nearest_index = np.argmin(iat_table['ABS_TIME'])\n",
    "\n",
    "    # Geting the quaternion\n",
    "    qparam = iat_table['QPARAM'][nearest_index]\n",
    "    iss2sky = R.from_quat(qparam)\n",
    "#    Should I keep these printouts?\n",
    "#    print('Input MET = %.6f' % (trigtime_met))\n",
    "#    print('Nearest MET = %.6f' % (iat_table['TIME'][nearest_index]))\n",
    "#    print('Time Diff. = %.6f' % (iat_table['TIME'][nearest_index]-trigtime_met))\n",
    "#    print()\n",
    "    return iss2sky\n",
    "\n",
    "###############################################################\n",
    "def get_cgbm_pointing(iss2sky, theta, phi, det):\n",
    "    \"\"\"\n",
    "        This function get CGBM pointing directions.\n",
    "\n",
    "        Args:\n",
    "            iss2sky (scipy.spatial.transform.Rotation): Rotation matrix from ISS\n",
    "            coordinate to the celestial coordinates.\n",
    "            theta (float) : zenith angle [deg]\n",
    "            phi (float) : azimuth [deg]\n",
    "            det (str): detector name (HXM1/HXM2/SGM)\n",
    "        Returns:\n",
    "            sky_pos (astropy.coordinates.SkyCoord): Sky position of the direction\n",
    "            which was represented by theta and phi in the detector coordinate.\n",
    "    \"\"\"\n",
    "\n",
    "    det = det.upper()\n",
    "\n",
    "    if theta < 0 or theta > 180:\n",
    "        raise ValueError(\"theta must be in the [0, 180] deg range.\")\n",
    "    if phi < 0 or phi > 360:\n",
    "        raise ValueError(\"phi must be in the [0, 360] deg range\")\n",
    "\n",
    "    # Convert position from spherical to cartesian coordinates.\n",
    "    lon = phi * u.deg\n",
    "    lat = (90.0 - theta) * u.deg\n",
    "    pos_cgbm = UnitSphericalRepresentation(lon, lat).to_cartesian()\n",
    "\n",
    "    # Rotation from the CGBM to the celestial coordinates\n",
    "    cgbm2sky = iss2sky * cg.CAL2ISS * cg.CGBM2CAL[det]\n",
    "\n",
    "    # Rotate the position\n",
    "    sky_pos = cgbm2sky.apply(pos_cgbm.xyz)\n",
    "\n",
    "    # Transfrom to SkyCoord with ra, dec\n",
    "    sky_pos = SkyCoord(sky_pos[0], sky_pos[1], sky_pos[2],\n",
    "                       representation_type='cartesian', frame='icrs')\n",
    "    sky_pos.representation_type = 'spherical'\n",
    "#    print('%s zenith (R.A., Dec.) = (%.3f, %.3f)' % (det , sky_pos.ra.deg, sky_pos.dec.deg))\n",
    "    return sky_pos\n",
    "\n",
    "###############################################################\n",
    "def get_cgbm_theta_phi(iss2sky, ra, dec, det):\n",
    "    \"\"\"\n",
    "        This function get the incident angle of the direction which is given as ra and dec.\n",
    "\n",
    "        Args:\n",
    "            iss2sky (scipy.spatial.transform.Rotation): Rotation matrix from ISS\n",
    "            coordinate to the celestial coordinates.\n",
    "            ra (float) : Right Ascension [deg]\n",
    "            dec (float) : Declination [deg]\n",
    "            det (str): detector name (HXM1/HXM2/SGM)\n",
    "        Returns:\n",
    "            pos_cgbm_deg ((float, float)): Incident angles (zenith angle [deg], azimuth [deg])\n",
    "    \"\"\"\n",
    "\n",
    "    sky2iss = iss2sky.inv()\n",
    "    det = det.upper()\n",
    "\n",
    "    pos_sky = SkyCoord(ra, dec, unit='deg', frame='icrs')\n",
    "\n",
    "    # Rotation from sky to CGBM coordinates\n",
    "\n",
    "    sky2cgbm = cg.CAL2CGBM[det] * cg.ISS2CAL * sky2iss\n",
    "\n",
    "    # `pos_sky.cartesian.xyz` yields a Quantity object, adding `.value`\n",
    "    # gives `numpy.ndarray` (though this will also work with Quantity)\n",
    "    pos_cgbm = sky2cgbm.apply(pos_sky.cartesian.xyz.value)\n",
    "    theta = np.arccos(pos_cgbm[2])\n",
    "    phi = np.arctan2(pos_cgbm[1], pos_cgbm[0])\n",
    "\n",
    "    # np.arctan2 yields an angle in the [-pi, pi] range, so we need to\n",
    "    # add 2*pi for negative values to get the [0, 2*pi] range\n",
    "\n",
    "    if phi < 0:\n",
    "        phi += 2 * np.pi\n",
    "    pos_cgbm_deg = (np.rad2deg(theta), np.rad2deg(phi))\n",
    "    return pos_cgbm_deg\n",
    "\n",
    "###############################################################\n",
    "def get_solar_incident_angle(iss2sky, trigtime_met, det):\n",
    "    \"\"\"\n",
    "        This function get the incident angle of the direction which is given as ra and dec.\n",
    "\n",
    "        Args:\n",
    "            iss2sky (scipy.spatial.transform.Rotation): Rotation matrix from ISS\n",
    "            coordinate to the celestial coordinates\n",
    "            trigtime_met (float): trigger time in MET\n",
    "            det (str): detector name (HXM1/HXM2/SGM)\n",
    "        Returns:\n",
    "            pos_cgbm_deg ((float, float)): Incident angles (zenith angle [deg], azimuth [deg]) of the sun\n",
    "            sun_ra_rad, sun_dec_rad\n",
    "    \"\"\"\n",
    "\n",
    "    trigtime_utc = met2utc(trigtime_met)\n",
    "\n",
    "    # Get solar position\n",
    "    sun = get_sun(trigtime_utc)\n",
    "    solar_angle = get_cgbm_theta_phi(iss2sky, sun.ra.deg, sun.dec.deg, det)\n",
    "\n",
    "#    print('%s solar angle (theta, phi) = (%.3f, %.3f)' % (det , solar_angle[0], solar_angle[1]))\n",
    "#    print ('get_solar_incident_angle: ',sun.ra.deg, sun.dec.deg)\n",
    "\n",
    "    sun_ra_rad = sun.ra.deg * np.pi/180.\n",
    "    sun_dec_rad = sun.dec.deg * np.pi/180.\n",
    "#    print ('get_solar_incident_angle in radians: ',sun_ra_rad, sun_dec_rad)\n",
    "\n",
    "    return solar_angle, sun_ra_rad, sun_dec_rad\n",
    "\n",
    "###############################################################\n",
    "def get_angles(data_dir, fileid, trigtime_met):\n",
    "    \"\"\"\n",
    "        This function calculates pointing directions and solar angles and print them.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): path to the data directory\n",
    "            fileid (str): date of the data (yyyymmdd)\n",
    "            trigtime_met (float): trigger time in MET\n",
    "        Returns:\n",
    "            ra_rad, sun_dec_rad, zenith_hxm.ra.deg, zenith_hxm.dec.deg, zenith_sgm.ra.deg, zenith_sgm.dec.deg, solar_hxm, solar_sgm\n",
    "    \"\"\"\n",
    "\n",
    "    iat_file = '%s/cgbm_%s.iat' % (data_dir, fileid)\n",
    "    iat_table = Table.read(iat_file, hdu=1)\n",
    "    iat_table['TIME'] = iat_table['TIME'] - cg.PERIODIC_HBIN\n",
    "\n",
    "    iss2sky = get_iss_att(trigtime_met, iat_table)\n",
    "    zenith_hxm = get_cgbm_pointing(iss2sky, 0., 0., 'HXM')\n",
    "    zenith_sgm = get_cgbm_pointing(iss2sky, 0., 0., 'SGM')\n",
    "\n",
    "    solar_hxm, sun_ra_rad, sun_dec_rad = get_solar_incident_angle(iss2sky, trigtime_met, 'HXM')\n",
    "    solar_sgm, sun_ra_rad, sun_dec_rad = get_solar_incident_angle(iss2sky, trigtime_met, 'SGM')    \n",
    "\n",
    "    return sun_ra_rad, sun_dec_rad, zenith_hxm.ra.deg, zenith_hxm.dec.deg, zenith_sgm.ra.deg, zenith_sgm.dec.deg, solar_hxm, solar_sgm\n",
    "\n",
    "###############################################################\n",
    "def process_trigger(data_dir, fileid, trigid, trigtime_met, trigtime_mdc):\n",
    "    \"\"\"\n",
    "        This function make plots and gets pointing directions & solar angles.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): path to the data directory\n",
    "            fileid (str): date of the data (yyyymmdd)\n",
    "            trigid (int): integer of the trigger time in MDC time\n",
    "            trigtime_met (float): trigger time in MET\n",
    "            trigtime_mdc (float): trigger time in MDC time\n",
    "        Returns:\n",
    "            sun_ra_rad, sun_dec_rad, hxm_ra_deg, hxm_dec_deg, sgm_ra_deg, sgm_dec_deg, solar_hxm, solar_sgm, lat,lon, UT\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    th_data = get_th_data(data_dir, fileid)\n",
    "\n",
    "    # Create CGBM light curves\n",
    "    # plt_lc_single_det(th_data, trigid, trigtime_met, 'HXM1')\n",
    "    # plt_lc_single_det(th_data, trigid, trigtime_met, 'HXM2')\n",
    "    # plt_lc_single_det(th_data, trigid, trigtime_met, 'SGM')\n",
    "\n",
    "    # Get the ISS orbit\n",
    "    lat, lon = plot_orb(data_dir, fileid, trigtime_met)\n",
    "    trigtime_utc = met2utc(trigtime_met)\n",
    "\n",
    "    # Printing the triggerID and trigger time\n",
    "    # print('TriggerID: %d' % (trigid))\n",
    "    # print('Trigger Time (MDC): %.6f' % (trigtime_mdc))\n",
    "    # print('Trigger Time (MET): %.6f' % (trigtime_met))\n",
    "    # print('Trigger Time (UT): %s' % (trigtime_utc.isot))\n",
    "\n",
    "    UT = trigtime_utc.isot\n",
    "    # Printing the pointing directions and solar angles\n",
    "    sun_ra_rad, sun_dec_rad, hxm_ra_deg, hxm_dec_deg, sgm_ra_deg, sgm_dec_deg, solar_hxm, solar_sgm = get_angles(data_dir, fileid, trigtime_met)\n",
    "    return sun_ra_rad, sun_dec_rad, hxm_ra_deg, hxm_dec_deg, sgm_ra_deg, sgm_dec_deg, solar_hxm, solar_sgm, lat,lon, UT\n",
    "\n",
    "###############################################################\n",
    "def Haversine(delta1, delta2, alpha1, alpha2):\n",
    "# routine to calculate difference between two locations on sky\n",
    "    arg1 = math.sin( .5*(delta2 - delta1))\n",
    "    arg2 = math.sin( .5*(alpha2 - alpha1))\n",
    "    arg = arg1 * arg1 + math.cos(delta1) * math.cos(delta2) *arg2 * arg2\n",
    "    theta = 2. * math.asin( math.sqrt(arg))\n",
    "# Astronomy cafe\n",
    "#    theta = math.acos ( math.sin(delta1) * math.sin(delta2) + math.cos(delta1) * math.cos(delta2) * math.cos(alpha1 - alpha2) )\n",
    "    return theta\n",
    "\n",
    "###############################################################\n",
    "def solar (fileid, time_UT, duration):\n",
    "# Solar activity reader from Henry Vo, 2/28/2024\n",
    "\n",
    "    # pip install bs4 \n",
    "    # pip install selenium \n",
    "    # pip install pandas\n",
    "    # pip install requests\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime, time, timedelta\n",
    "\n",
    "    # Create a webdriver object to automate chrome\n",
    "    driver = webdriver.Chrome()\n",
    "    # Open the solar activity webpage\n",
    "    driver.get('https://www.polarlicht-vorhersage.de/goes-archive/')\n",
    "\n",
    "    # Prompt the user for date, time, and duration of the graph\n",
    "    # date = input('Enter the date of the desired event (yyyymmdd)')\n",
    "    # fileid = '20231201'\n",
    "    date = fileid\n",
    "    # time_UT = input('Enter the time of the desire event (UT) (HHMM)')\n",
    "    time_UT = \"1200\"\n",
    "    # duration = input(\"Enter the desired time duration\")\n",
    "    duration = \"12\"\n",
    "\n",
    "### Filter the graph to be in the desired time window\n",
    "#### First, change the date to time window\n",
    "# Find all the input dropboxes from the html\n",
    "    elements = driver.find_elements(By.XPATH, '//input')\n",
    "\n",
    "    min_date_element = elements[0] # First date dropbox html element\n",
    "    min_time_element = elements[1] # First time dropbox html element\n",
    "    max_date_element = elements[2] # Second date dropbox html element\n",
    "    max_time_element = elements[3] # Second time dropbox html element\n",
    "\n",
    "### Take the inputted date and convert to a usable date and time that can be subtracted and added by the time duration\n",
    "    date = datetime.strptime(date, \"%Y%m%d\") \n",
    "\n",
    "    hour = time_UT[:2]\n",
    "    minute = time_UT[2:]\n",
    "\n",
    "    time_UT = time(int(hour), int(minute), 0)\n",
    "    datetime_obj = datetime.combine(date, time_UT) #Create a datetime object with the date and time\n",
    "\n",
    "    max_datetime = datetime_obj+timedelta(hours=int(duration)) # Add time duration +- to the datetime object\n",
    "    min_datetime = datetime_obj-timedelta(hours=int(duration))\n",
    "\n",
    "    max_date = max_datetime.strftime(\"%m/%d/%Y\")\n",
    "    min_date = min_datetime.strftime(\"%m/%d/%Y\")\n",
    "    max_time = max_datetime.strftime(\"%I:%M %p\")\n",
    "    min_time = min_datetime.strftime(\"%I:%M %p\")\n",
    "\n",
    "# Clear all the dropboxes of any information\n",
    "    min_date_element.clear() \n",
    "    min_time_element.clear()\n",
    "    max_date_element.clear()\n",
    "    max_time_element.clear()\n",
    "\n",
    "# Insert the desired values into the dropboxes\n",
    "    min_date_element.send_keys(min_date)\n",
    "    min_time_element.send_keys(min_time)\n",
    "    max_date_element.send_keys(max_date)\n",
    "    max_time_element.send_keys(max_time)\n",
    "\n",
    "# Print the image\n",
    "    graph_element = driver.find_elements(By.XPATH, '//img')[2]\n",
    "\n",
    "# Take the link from the 'src' attribute in the html\n",
    "    graph = graph_element.get_attribute('src')\n",
    "\n",
    "# Open the link and display it as a .png\n",
    "    response = requests.get(graph)\n",
    "    image_date = response.content\n",
    "    image = Image.open(BytesIO(image_date))\n",
    "#    image.show()\n",
    "    image.save('Plot_solar.pdf')\n",
    "    image.close ()\n",
    "\n",
    "# tmp0oy0f1dj.png\n",
    "\n",
    "# Close out of the webpage\n",
    "    driver.quit()\n",
    "\n",
    "    return\n",
    "\n",
    "###############################################################\n",
    "# X-Ray Flux Data Retriever.py -- Henry Vo,5/21/24\n",
    "# Import the libraries\n",
    "\n",
    "def Xray_flux1 (fileid, UT):\n",
    "    import os\n",
    "    import requests\n",
    "    import xarray as xr\n",
    "    from bs4 import BeautifulSoup\n",
    "    import netCDF4 as nc\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import sys\n",
    "\n",
    "    print ('*** 5a Enter X-ray')\n",
    "# Take the input date and sort into year, month, day\n",
    "\n",
    "# Input the trigger date\n",
    "#    trig_date = input(\"Enter the date with a format: yyyymmdd\") + \"T00:00:00Z\"\n",
    "    trig_date = fileid + \"T00:00:00Z\"\n",
    "\n",
    "# Split the date into year, month, and day by indexing the date string\n",
    "    year = trig_date[:4]\n",
    "    month = trig_date[4:6]\n",
    "    day = trig_date[6:8]\n",
    "\n",
    "# Determine current directory\n",
    "#    print(os.getcwd() + \"\\n\")\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "# Determine which GOES contains the data\n",
    "    if trig_date[:4] == '2022':\n",
    "        if month in ('09,10,11,12'):\n",
    "            goes = 'goes18'\n",
    "        else:\n",
    "            goes = 'goes17'\n",
    "    elif trig_date[:4] in ('2024,2023'):\n",
    "        goes = 'goes18'\n",
    "    elif trig_date[:4] in ('2018,2019,2020,2021'): \n",
    "        goes = 'goes17'\n",
    "    else:\n",
    "        goes = 'goes16'\n",
    "\n",
    "# Find the link of the year and the date suggested\n",
    "    base_link = 'https://data.ngdc.noaa.gov/platforms/solar-space-observing-satellites/goes/' + goes + '/l2/data/xrsf-l2-flx1s_science/' + year +'/' + month + '/'\n",
    "\n",
    "# Get the HTML content of the webpage\n",
    "    response = requests.get(base_link)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# ## Look  through the <\\a> tags in the  html and look for the link of the target date, then download the nc file\n",
    "\n",
    "    a_links = soup.find_all('a')\n",
    "\n",
    "# Loop through each <a> tag ang find the link to download the nc file\n",
    "    target_link = \"\"\n",
    "    for link in a_links[4:-1]:\n",
    "        if len (link['href'].split('_')) > 3:\n",
    "            if day == link['href'].split('_')[3][7:9]:\n",
    "#                print ('*** day = ')\n",
    "                target_file = link.get('href')\n",
    "                target_link = base_link + link.get('href')\n",
    "    if target_link == \"\":\n",
    "        Average = Variance = GOES_avg_1hr = GOES_var_1hr = -1\n",
    "        return Average, Variance, GOES_avg_1hr, GOES_var_1hr\n",
    "\n",
    "#    print ('*** 5b, link   ')\n",
    "#    print (' 5b, link again  ', target_link)\n",
    "\n",
    "# Get the content from the target_link\n",
    "    response = requests.get(target_link)\n",
    "#    print ('*** 5b0')\n",
    "# ## Write the nc file to a local directory named nc_files\n",
    "\n",
    "# #### First check to see if a nc_files directory already exists, if not make one\n",
    "\n",
    "# Set the base of the path where the nc files should be stored\n",
    "#base = r'C:\\Users\\henry\\OneDrive - Louisiana State University\\Laptop Desktop\\Research\\Jupyter'\n",
    "#    print ('*** 5b1')\n",
    "    base = r'C:\\Users\\Owner\\code\\classifier\\classify\\Xray'\n",
    "#    print ('*** 5b2')\n",
    "\n",
    "# Join the base of the path with the file name 'nc_files'\n",
    "    flux_data_path = os.path.join(base, 'nc_files')\n",
    "\n",
    "# Make a nc_files directory if one doesnt exist. If it already exists, change the working directory to the nc_files directory\n",
    "    if not os.path.exists(flux_data_path):\n",
    "      os.makedirs(flux_data_path)\n",
    "    else:\n",
    "        os.chdir(flux_data_path)\n",
    "#    print ('*** 5b3')\n",
    "\n",
    "#### Now write the nc_file to this directory\n",
    "\n",
    "# Generate a unique filename based on the current date and time\n",
    "    timestamp = str(year +  month + day)\n",
    "    nc_file_path = f\"flux_data_{timestamp}.nc\" # Set the name of the file as data_yyyymmdd \n",
    "\n",
    "\n",
    "    cwd_files = os.listdir('.') # Get all the files inside the nc_files directory\n",
    "# If the file does not already exist in the nc_files folder, write an nc.file\n",
    "\n",
    "    if nc_file_path not in cwd_files:\n",
    "        with open(nc_file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "# Open the NetCDF file\n",
    "    nc_file = nc.Dataset(nc_file_path)\n",
    "\n",
    "##     *****************************                Calculate the average counts and variance               ************************\n",
    "#    print ('*** 5b4')\n",
    "\n",
    "    flux = nc_file.variables['xrsa_flux'][:]\n",
    "    Average = np.average(flux)\n",
    "    Variance = np.sqrt(np.var(flux) / (86400-1))\n",
    "\n",
    "    print (\"UT:  \",UT, UT[11:13], UT[14:16])\n",
    "    trigger_time_in_secs = int(UT[11:13]) * 3600 + int(UT[14:16]) * 60\n",
    "    print (trigger_time_in_secs)\n",
    "    start_time = trigger_time_in_secs - 3600\n",
    "    end_time = trigger_time_in_secs + 3600\n",
    "    GOES_avg_1hr = np.average(flux[start_time:end_time])\n",
    "    GOES_var_1hr = np.sqrt(np.var(flux[start_time:end_time]) / (7200-1))\n",
    "    print (\"Fluxes:  \", Average, Variance, GOES_avg_1hr, GOES_var_1hr)\n",
    "\n",
    "#    print(\"Average_Flux: \", Average, \"      Variance: \", Variance)\n",
    "\n",
    "## Now plot the data from the newly written nc_file \n",
    "\n",
    "# Change directory into the one where XFlux_plotter.py is located\n",
    "#os.chdir(r'C:\\Users\\henry\\OneDrive - Louisiana State University\\Laptop Desktop\\Research\\Jupyter')\n",
    "    os.chdir(r'C:\\Users\\Owner\\code\\classifier\\classify\\Xray')\n",
    "    print ('*** 5c')\n",
    "\n",
    "\n",
    "# Import the plot_values function from XFlux_plotter.py\n",
    "# from XFlux_plotter import plot_xflux\n",
    "\n",
    "# Call the function to plot the data\n",
    "# plot_xflux(nc_file)\n",
    "\n",
    "    \"\"\"\n",
    "XFlux_plotter.py\n",
    "\n",
    "This function takes one argument:\n",
    "1) A nc_file object from the netCDF4 library\n",
    "\n",
    "The function takes the values from the nc file and plots it over a 24 hr window. \n",
    "    \"\"\"\n",
    "# def plot_xflux(nc_file):\n",
    "#    print ('*** 5d')\n",
    "    import netCDF4 as nc\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Extract which goes from the ncfile\n",
    "    goes = nc_file.platform\n",
    "\n",
    "    # Set the y variable to xrayflux\n",
    "    flux = nc_file.variables['xrsa_flux'][::5]\n",
    "\n",
    "    # Set the x variable to time\n",
    "    start_time = datetime.strptime(nc_file.time_coverage_start, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "    # Create an array of 17280 datetime elements, starting from 0 to 86400-1 that represents every 5 seconds\n",
    "    array_of_seconds = np.arange(0.0, 86400.0-1.0, 5)                                      \n",
    "\n",
    "    time_index = [start_time + timedelta(seconds=i) for i in array_of_seconds]    \n",
    "\n",
    "    # Plot the xrayflux(y) vs. time_index(x)\n",
    "    plt.figure(figsize=(10,8)) # Set the size of the graph\n",
    "    plt.plot(time_index, flux)  \n",
    "\n",
    "    plt.yscale('log')\n",
    "#    plt.title(goes.upper() + ' X-ray Flux vs. Time   (Trigger time: ' + UT + ')') # Set the title of the plot\n",
    "    plt.title('GOES X-ray flux vs. time        Trigger time (UT): ' + UT)\n",
    "    plt.ylim(1e-8, None)\n",
    "    plt.yticks([1e-9, 1e-8, 1e-7,1e-6]) # Set the y axis tick markers\n",
    "\n",
    "# Switch back to original directory to store plot fileand continue program execution\n",
    "    os.chdir(current_dir)\n",
    "    plt.savefig (\"GOES_flux.pdf\")\n",
    "\n",
    "#    plt.show()\n",
    "    plt.close ('all')\n",
    "    if GOES_avg_1hr > 0 and GOES_var_1hr > 0:\n",
    "        print ('***5e GOES values: ', Average, Variance, GOES_avg_1hr, GOES_var_1hr)\n",
    "    else:\n",
    "#        Average = Variance = GOES_avg_1hr = GOES_var_1hr = -1\n",
    "        GOES_avg_1hr = GOES_var_1hr = -1\n",
    "        print ('***5f GOES values: ', Average, Variance, GOES_avg_1hr, GOES_var_1hr)\n",
    "\n",
    "    if Average == 'nan':\t\t# Check that GOES values are not 'nan'\n",
    "        Average = -1\n",
    "    if Variance == 'nan':\n",
    "        Variance = -1\n",
    "    if GOES_avg_1hr == 'nan':\n",
    "        GOES_avg_1hr = -1\n",
    "    if GOES_var_1hr == 'nan':\n",
    "        GOES_var_1hr = -1\n",
    " \n",
    "    return Average, Variance, GOES_avg_1hr, GOES_var_1hr\n",
    "\n",
    "###############################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "def analyze_single_trigger (fileid, chan_type, data_dir, ph_table, trig_index, trig_time, trig_num, triggers,\n",
    "event_id_rows, event_UT_rows, event_type_rows, tmin, tmax):\n",
    "#    This is basically analyze8.py\n",
    "    trigtime_met = ref_MET = trig_time[trig_num]  #  Multiple triggers?   # These three lines ensure compatibility with process_trigger\n",
    "    trigtime_mdc = trig_index[trig_num]\n",
    "    trigid = int(trigtime_mdc)\n",
    "\n",
    "    ph_table['ZTIME'] = ph_table['TIME'] - cg.PH_HBIN - ref_MET\n",
    "\n",
    "################################\n",
    "# Find nearest hv turn on/off time\n",
    "# Revised version -- Search over +-10800 s = 2700 time bins -- 6/17/24\n",
    "    \"\"\"\n",
    "    threshold = 2000\n",
    "    ir_on = ir_off = transition = turn_off = turn_on = -86400\n",
    "\n",
    "    ir_end = len(ph_table['ZTIME'])\n",
    "    for ir in range (1,ir_end):\n",
    "        if ph_table['ZTIME'][ir] > -4000:\t\t\t# Start search for transition at 4000 before trigger\n",
    "            ir_start = ir + 1\n",
    "            break\n",
    "    for ir in range (ir_start,min(ir_start+2000,ir_end)):\n",
    "        print (i2, ph_table['ZTIME'][ir])\n",
    "#        xx1 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "#        xx2 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir-1])\n",
    "        xx1 = int(np.sum(ph_table['SGML_COUNTS'], axis=1)[ir])\n",
    "        xx2 = int(np.sum(ph_table['SGML_COUNTS'], axis=1)[ir-1])\n",
    "\n",
    "        if (xx1-xx2) > threshold:\t\t\t\t# Starting at ir = -2000, is difference between two successive records > 2000?\n",
    "            turn_on = ph_table['ZTIME'][ir]\n",
    "            ir_on = ir\t\t\t\t\t\t# Identify index ir at which turn_on occurs\n",
    "        if (xx2-xx1) > threshold:\n",
    "            turn_off = ph_table['ZTIME'][ir]\n",
    "            ir_off = ir\t\t\t\t\t\t# Identify index ir at which turn_off occurs\n",
    "        if turn_on > -86400 and turn_off > -86400 :\t\t# Have I found both an up and a down transition within 2000 counts of trigger?\n",
    "            break\n",
    "    if abs(turn_on) > abs(turn_off):\n",
    "        transition = int(turn_off)\t\t\t\t# Transition = time of turn on/off, ir_off/ir_on = index\n",
    "        ir = ir_off\t\t\t\t\t\t#    - ==> turn on, + ==> turn off\n",
    "    else:\t\t\t\t\t\t\t#    -86400 means turn on at very start of event or never found a transition\n",
    "        transition = int(turn_on)\n",
    "        ir = ir_on\n",
    "    \"\"\"\n",
    "    threshold = 1000\n",
    "    imax= len(ph_table['ZTIME'])\n",
    "#    print ('Start time: ', ph_table['ZTIME'][0], '  End time: ',ph_table['ZTIME'] [imax-1] )\n",
    "    transition = 86400\n",
    "    ir_transition = 0\n",
    "    ir_start = 0\n",
    "    ir_end = len(ph_table['ZTIME'])\n",
    "#    for ir in range (ir_start, ir_end,5):\n",
    "#        print ('@@@@',ir, ph_table['ZTIME'][ir], np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "    ir_start = -float(ph_table['ZTIME'][0])/4 - 2700\t\t# Start 1/2 orbit prior to trigger, extend to 1/2 orbit after\n",
    "    ir_start = int(ir_start)\n",
    "    ir_end = ir_start + 5400\n",
    "    for ir in range (ir_start, ir_end):\n",
    "        xx1 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "        xx2 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir-1])\n",
    "        if abs(xx1-xx2) > threshold:\t\t\t\t# Starting at ir = -2700, is difference between two successive records > 2000?\n",
    "            turn_on = int(ph_table['ZTIME'][ir])\t\t# Tentative time at which transition occurs\n",
    "            if abs(turn_on) < abs(transition):\n",
    "                transition = turn_on\t\t\t\t# Identify time at which closest turn_on/off occurs\n",
    "                ir_transition = ir\t\t\t\t# Identify index at which closest turn_on/off occurs\n",
    "\n",
    "    print ('Time to turn on/off (sec) = ',transition)\n",
    "\n",
    "#   Print counts vs time around hv transition\n",
    "    ir_first = ir_transition - 10\n",
    "    if (ir_first < ir_start):\n",
    "        ir_first = ir_start\n",
    "    ir_last = ir_first + 20\n",
    "    if (ir_last > ir_end):\n",
    "        ir_last = ir_end\n",
    "    if ir_first <= ir_last :\n",
    "        print ('Counts around hv turn on/off transition:')\n",
    "        for i in range (ir_first, ir_last):\n",
    "            print(' HXM1L counts near hv transition: ', ph_table['ZTIME'][i],np.sum(ph_table['HXM1L_COUNTS'], axis = 1)[i])\n",
    "        print ()\n",
    "#    print ('*** Finished 1')\n",
    "\n",
    "################################\n",
    "# Plot distribution of counts vs time \n",
    "    time_dist(ph_table['ZTIME'], ph_table['HXM1L_COUNTS'], ph_table['HXM2L_COUNTS'], ph_table['SGML_COUNTS'], ref_MET, fileid, trig_num)\n",
    "   \n",
    "################################\n",
    "# Fit counts vs time plus background -- Gaussian\n",
    "    ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1, d, e, f = fitter(ph_table['ZTIME'], ph_table['SGML_COUNTS'], ref_MET, tmin, tmax)\n",
    "# Fit counts vs time plus background -- Norris function\n",
    "\n",
    "    ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2 = fitter_Norris(ph_table['ZTIME'], \n",
    "        ph_table['SGML_COUNTS'], ref_MET, peak_pos1, FWHM, d, e, f)\n",
    "#    print ('***** 2 Return from Norris fit')\n",
    "    \n",
    "################################\n",
    "# Plot spectra vs energy\n",
    "#   Use default start and stop time\n",
    "\n",
    "#    tmin = float(input(\"Please input the start time:\"))\n",
    "#    tmax = float(input(\"Please input the end time:\"))\n",
    "#    print ()\n",
    "#    tmin = -200\n",
    "#    tmax = 200\n",
    "    tmask = (tmin <= ph_table['ZTIME']) & (ph_table['ZTIME'] <= tmax)\n",
    "    roi = ph_table[tmask]\n",
    "    row_num = float(len(roi))\n",
    "\n",
    "    detectors = ['HXM1H', 'HXM2H', 'SGMH', 'HXM1L', 'HXM2L', 'SGML']\n",
    "    id = 0  \n",
    "    sumplot = np.zeros (len(detectors))\n",
    "    for detector in detectors:\n",
    "        y = np.sum(roi[detector + '_RATE'], axis = 0)\n",
    "        if (detector == 'HXM1H' or detector == 'HXM2H' or detector == 'SGMH'):      # Change x, z values for first 3 high-threshold detectors\n",
    "            Emin = 20\n",
    "            Emax = 100\n",
    "            x = np.arange(cg.PH_BINNUM_HIGH)\n",
    "            fig = plt.figure(figsize=(8, 6))\n",
    "            plt.plot( x, y / cg.high_ph_width() / row_num, drawstyle='steps-mid')\n",
    "        else:                                                                       # Change x, z values for last 3 low-threshold detectors\n",
    "            Emin = 10\n",
    "            Emax = 400\n",
    "            x = np.arange(cg.PH_BINNUM_LOW)\n",
    "            fig = plt.figure(figsize=(8, 6))\n",
    "            plt.plot( x, y / cg.low_ph_width() / row_num, drawstyle='steps-mid')\n",
    "\n",
    "#    Calculate hardness ratio between Emin and Emax separately for low- and high-threshold detectors\n",
    "        for i in range (0, len(x)):        # Sum up counts in appropriate energy ranges\n",
    "            if (x[i] > Emin) & (x[i] < Emax):\n",
    "                sumplot[id] += y[i]\n",
    "   \n",
    "        id += 1\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('PH channel')\n",
    "        plt.ylabel('Counts /s /ADC channel')\n",
    "        plt.title(detector + ' spectrum')\n",
    "        fig.savefig ('plot3'+str(id)+'.pdf')\n",
    "#       plt.show()\n",
    "        plt.close (fig)\n",
    "\n",
    "#    print ('**** 3 Return from plotting spectra vs energy')\n",
    "\n",
    "################################\n",
    "#     Hardness ratios\n",
    "    HXM1_ratio = sumplot[3]/sumplot[0]    \n",
    "    HXM2_ratio = sumplot[4]/sumplot[1]\n",
    "    SGM_ratio  = sumplot[5]/sumplot[2]\n",
    "#    print('Hardness ratios --      HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "#    print ('****   4  Return from hardness ratio calculation')\n",
    "\n",
    "################################\n",
    "#     determine latitude\n",
    "#    plot_orb(data_dir, fileid, ref_MET)     # Multiple trigger times?\n",
    "\n",
    "################################\n",
    "#    determine latitude and angle to Sun\n",
    "    sun_ra_rad, sun_dec_rad, hxm_ra_deg, hxm_dec_deg, sgm_ra_deg, sgm_dec_deg, solar_hxm, solar_sgm, lat, lon, UT = process_trigger(\n",
    "          data_dir, fileid, trigid, trigtime_met, trigtime_mdc)\n",
    "    if (sun_dec_rad) < 0 :\n",
    "        sun_dec_rad += 2.*math.pi\n",
    "\n",
    "    hxm_ra_rad = hxm_ra_deg * math.pi/180.\n",
    "    hxm_dec_rad = hxm_dec_deg * math.pi/180.\n",
    "    sgm_ra_rad = sgm_ra_deg * math.pi/180.\n",
    "    sgm_dec_rad = sgm_dec_deg * math.pi/180.\n",
    "\n",
    "    alpha1 = sun_ra_rad\n",
    "    alpha2 = hxm_ra_rad\n",
    "    alpha3 = sgm_ra_rad\n",
    "    dec1 = sun_dec_rad\n",
    "    dec2 = hxm_dec_rad\n",
    "    dec3 = sgm_dec_rad\n",
    "\n",
    "# Calculation between 2 points with RA, dec using Haversine formula\n",
    "    theta12 = Haversine (dec1, dec2, alpha1, alpha2) * 180./math.pi\n",
    "    theta13 = Haversine (dec1, dec3, alpha1, alpha3) * 180./math.pi\n",
    "\n",
    "# Calculation between 2 points with theta, phi using dot product of vectors\n",
    "    theta1 = phi1 = 0.\n",
    "    theta2 = solar_hxm[0] * math.pi/180.\n",
    "    phi2 = solar_hxm[1] * math.pi/180.\n",
    "    delta_theta_hxm = math.acos( math.sin(theta1) * math.cos(phi1) * math.sin(theta2) * math.cos(phi2) \n",
    "                          + math.sin(theta1) * math.sin(phi1) * math.sin(theta2) * math.sin(phi2)\n",
    "                          + math.cos(theta1) * math.cos(theta2) ) * 180./math.pi\n",
    "    theta3 = solar_sgm[0] * math.pi/180.\n",
    "    phi3 = solar_sgm[1] * math.pi/180.\n",
    "    delta_theta_sgm = math.acos( math.sin(theta1) * math.cos(phi1) * math.sin(theta3) * math.cos(phi3) \n",
    "                          + math.sin(theta1) * math.sin(phi1) * math.sin(theta3) * math.sin(phi3)\n",
    "                          + math.cos(theta1) * math.cos(theta3) ) * 180./math.pi\n",
    "\n",
    "################################\n",
    "#     Solar activity\n",
    "\n",
    "    global GOES_avg, GOES_var\n",
    "    duration = \"1200\"\n",
    "    GOES_avg = GOES_var = -1\n",
    "#    print('*** 5 Enter solar', trig_num, GOES_avg, GOES_var)\n",
    "    if trig_num == 0 or trig_num > 0:\n",
    "        GOES_avg, GOES_var, GOES_avg_1hr, GOES_var_1hr = Xray_flux1 (fileid, UT)\n",
    "        try: \n",
    "            solar (fileid, UT, duration)\n",
    "        except:\n",
    "            duration = '1200'\n",
    "    print ('**** 6 Trig_num =', trig_num, ' Return from solar', GOES_avg, GOES_var, GOES_avg_1hr, GOES_var_1hr)\n",
    "################################\n",
    "\n",
    "    \"\"\"\n",
    "# As of 2/22/2004 -- Calculate likelihoods for -- old version\n",
    "#        lat, transition, angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0]), FWHM, t90\n",
    "# To add categories, add them to data_types list in main routing and likelihood_list here\n",
    "# data_types, x, Ngtx, event_classes are global variables\n",
    "# data_types = ['Latitude','Time_to_transition','angle_to_Sun','FWHM','t90']\t-- Copy of data_types from main routine\n",
    "# event_classes = ['GRB', 'Solar','Particle']\t\t\t\t-- Copy of event classes from main routine\n",
    "# Ngtx [itype][events][row_number] = int (Ngtx [itype][events][row_number]) / norm [events]  --  Copy from main routine\n",
    "\n",
    "    angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0])\n",
    "    likelihood_list = [lat, transition, angle_to_Sun, FWHM, ampl1, chisq1, t90, ampl2, chisq2, SGM_ratio, GOES_avg, GOES_var, \n",
    "        GOES_avg_1hr, GOES_var_1hr]\n",
    "\n",
    "    LGRB = [0] * len(data_types)\n",
    "    Lsolar =[0] * len(data_types)\n",
    "    Lpart = [0] * len(data_types)\n",
    "    for idata in range (len(data_types)):\n",
    "        for ievent in range (len(event_classes)):\n",
    "            total = 0\n",
    "            if ievent == 0:\n",
    "                LGRB [idata] = 0 \n",
    "            if ievent == 1:\n",
    "                Lsolar [idata] = 0\n",
    "            if ievent == 2:\n",
    "                Lpart [idata] = 0\n",
    "            jrow = 0\n",
    "            while (jrow < max_rows):\n",
    "                jrow += 1\n",
    "#                print ('lat = ', lat, likelihood_list [idata], type(likelihood_list [idata]))\n",
    "#                print ('Ngtx ', idata, ievent, jrow, Ngtx [idata][ievent][jrow], type (Ngtx [idata][ievent][jrow]))  \n",
    "#                print ('xL ', idata, ievent, jrow, xL [idata][ievent][jrow], type (xL [idata][ievent][jrow]))\n",
    "#                print ('@@@@@$$$$$  ', idata, ievent, jrow, xL [idata][ievent][jrow], Ngtx [idata][ievent][jrow],\n",
    "#                    type(xL [idata][ievent][jrow]), type(Ngtx [idata][ievent][jrow]) )\n",
    "                if likelihood_list [idata] < float(xL [idata][ievent][jrow]) or Ngtx [idata][ievent][jrow] == 0:\n",
    "                    jrow -= 1\n",
    "                    if ievent == 0:\n",
    "                        LGRB [idata] = Ngtx [idata][ievent][jrow] \n",
    "                    if ievent == 1:\n",
    "                        Lsolar [idata] = Ngtx [idata][ievent][jrow] \n",
    "                    if ievent == 2:\n",
    "                        Lpart [idata] = Ngtx [idata][ievent][jrow]  \n",
    "                    total += Ngtx [idata][ievent][jrow]  \n",
    "                    jrow = 10000 \n",
    "#            continue\n",
    "#        print ('***** 9c ', idata, ievent)\n",
    "        LGRB [idata] = LGRB [idata] / total\n",
    "        Lsolar [idata] = Lsolar [idata] / total\n",
    "        Lpart [idata] = Lpart [idata] / total    \n",
    "\n",
    "    sum_data_types = [0] * len(event_classes)\t\t\t\t\t\t# Calculate Sum (L) and Product (ln L)\n",
    "    sum_log_data_types = [1] * len(event_classes)\n",
    "    count_sum = [0] * len(event_classes)\n",
    "    for idata in range (len(data_types)):\n",
    "        sum_data_types [1] += Lsolar [idata]\n",
    "        sum_data_types [2] += Lpart [idata]\n",
    "        if LGRB [idata] > 0:\n",
    "            sum_data_types [0] += LGRB [idata]\n",
    "            sum_log_data_types [0] = sum_log_data_types [0] + math.log (LGRB[idata])\n",
    "            count_sum [0] += 1\n",
    "         if Lsolar [idata] > 0:\n",
    "            sum_data_types [1] += Lsolar [idata]\n",
    "            sum_log_data_types [1] = sum_log_data_types [1] + math.log (Lsolar[idata])\n",
    "            count_sum [1] += 1\n",
    "         if Lpart [idata] > 0:\n",
    "            sum_data_types [2] += Lpart [idata]\n",
    "            sum_log_data_types [2] = sum_log_data_types [2] + math.log (Lpart[idata])\n",
    "            count_sum [02 += 1\n",
    "     \n",
    "    for ievent in range (len(event_classes)):\t\t\t\t\t\t# Normalize by number of non-zero likelihoods\n",
    "#        sum_data_types [ievent] = sum_data_types [ievent] / len (data_types)              \n",
    "        sum_data_types [ievent] = sum_data_types [ievent] / count_sum [ievent]\n",
    "        sum_log_data_types [ievent] = - math.exp (sum_log_data_types [ievent] ** (1 / count_sum [ievent] ))\n",
    "\n",
    "#    print ('*** 10  End likelihoods, enter print section, trig_num = ', trig_num)\n",
    "    \"\"\"\n",
    "\n",
    "##################################################################    \n",
    "\n",
    "# As of 6/5/2024 -- Calculate likelihoods for -- New version\n",
    "#        lat, transition, angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0]), FWHM, t90\n",
    "# To add categories, add them to data_types list in main routing and likelihood_list here\n",
    "# data_types, x, Ngtx, event_classes are global variables\n",
    "# data_types = ['Latitude','Time_to_transition','angle_to_Sun','FWHM','t90']\t-- Copy of data_types from main routine\n",
    "# event_classes = ['GRB', 'Solar','Particle']\t\t\t\t-- Copy of event classes from main routine\n",
    "\n",
    "    angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0])\n",
    "    likelihood_list = [lat, transition, angle_to_Sun, FWHM, ampl1, chisq1, t90, ampl2, chisq2, SGM_ratio, GOES_avg, GOES_var,\n",
    "        GOES_avg_1hr, GOES_var_1hr]\n",
    "    L_ratio = [0] * len(event_classes)\n",
    "\n",
    "    LGRB = [0] * len(data_types)\n",
    "    Lsolar =[0] * len(data_types)\n",
    "    Lpart = [0] * len(data_types)\n",
    "#    for idata in range (len(data_types)):\n",
    "    for idata in range (len(data_types)):\n",
    "        x = float( likelihood_list [idata])\n",
    "        if idata == 2: \t\t\t\t# Normalizations\n",
    "            arg = math.sin (x *3.14159265/180.)\n",
    "            x = arg * arg\n",
    "        if idata == 10:\t\t\t\t\n",
    "            x = x * 1.0e8\n",
    "        if idata == 11:\t\t\t\t\n",
    "            x = x * 1.0e12\n",
    "        \n",
    "        total = 0\n",
    "        for iclass in range (len(event_classes)):\n",
    "#            for bin in range (0,10):\n",
    "#                print (likelihood_list [idata], midpoints [idata] [bin] )\n",
    "#                print (likelihood [idata] [iclass] [bin])\n",
    "            for bin in range (1, max_bins):\n",
    "                L_ratio [iclass] = 0\n",
    "#                if likelihood_list [idata] > float(midpoints [idata] [max_bins - 1]):\n",
    "                if x > float(midpoints [idata] [max_bins - 1]):\n",
    "                    break\n",
    "#                if likelihood_list [idata] <= float(midpoints [idata] [bin]):\n",
    "                if x <= float(midpoints [idata] [bin]):\n",
    "#                    x = float( likelihood_list [idata])\n",
    "                    x2 =float( midpoints [idata] [bin])\n",
    "                    x1 = float( midpoints [idata] [bin-1])\n",
    "                    if x < x1:\n",
    "                        break\n",
    "                    y2 = float( likelihood [idata] [iclass] [bin] )   \n",
    "                    y1 = float( likelihood [idata] [iclass] [bin-1])  \n",
    "                    L_ratio [iclass] = (y2 - y1) / (x2 - x1) * (x - x1) + y1\n",
    "#                    if idata == 2:\n",
    "#                        print (x, x1, x, x2, y1, y2, L_ratio [iclass])\n",
    "                    break\n",
    "            total += L_ratio [iclass]\n",
    "        for iclass in range (len(event_classes)):\n",
    "            if total != 0:\n",
    "                L_ratio [iclass] = L_ratio [iclass] / total    # For each event, L_ratio is GRB/solar/particle likelihood for particular data class\n",
    "#        print (total, L_ratio [0], L_ratio [1], L_ratio [2])\n",
    "        LGRB [idata] = L_ratio [0] \n",
    "        Lsolar [idata] = L_ratio [1]\n",
    "        Lpart [idata] = L_ratio [2] \n",
    "#        sys.exit()\n",
    "#        print ('@@@@@',LGRB[idata], Lsolar[idata], Lpart[idata], total)\n",
    "\n",
    "    sum_data_types = [0] * len(event_classes)\t\t\t\t\t\t# Calculate Sum (L) and Product (ln L)\n",
    "    sum_log_data_types = [1] * len(event_classes)\n",
    "    count_sum = [0] * len(event_classes)\n",
    "    for idata in range (len(data_types)):\n",
    "        sum_data_types [1] += Lsolar [idata]\n",
    "        sum_data_types [2] += Lpart [idata]\n",
    "        if LGRB [idata] > 0:\n",
    "            sum_data_types [0] += LGRB [idata]\n",
    "            sum_log_data_types [0] = sum_log_data_types [0] + math.log (LGRB[idata])\n",
    "            count_sum [0] += 1\n",
    "        if Lsolar [idata] > 0:\n",
    "            sum_data_types [1] += Lsolar [idata]\n",
    "            sum_log_data_types [1] = sum_log_data_types [1] + math.log (Lsolar[idata])\n",
    "            count_sum [1] += 1\n",
    "        if Lpart [idata] > 0:\n",
    "            sum_data_types [2] += Lpart [idata]\n",
    "            sum_log_data_types [2] = sum_log_data_types [2] + math.log (Lpart[idata])\n",
    "            count_sum [2] += 1\n",
    "     \n",
    "    for ievent in range (len(event_classes)):\t\t\t\t\t\t# Normalize by number of non-zero likelihoods\n",
    "#        sum_data_types [ievent] = sum_data_types [ievent] / len (data_types)              \n",
    "        sum_data_types [ievent] = sum_data_types [ievent] / count_sum [ievent]\n",
    "        sum_log_data_types [ievent] = math.exp (sum_log_data_types [ievent]) ** (1 / count_sum [ievent] )\n",
    "\n",
    "################################\n",
    "#     Print out results\n",
    "\n",
    "#    if trig_num > 0:\n",
    "#        summary_file.write (summary_file_name + '   written by   ' + os.path.basename(sys.argv[0]) + '   ' + str(now) + '\\n\\n') \n",
    "    print('\\n %d of %d trigger(s) in %s ' % (trig_num + 1, len(triggers), fileid), end = '   --   ')   # print number of triggers\n",
    "\n",
    "    print('TriggerID: %d     \\nTrigger Time (MDC): %.6f     Trigger Time (MET): %.6f     Trigger Time (UT): %s' \n",
    "          % (trigid, trigtime_mdc, trigtime_met, UT))\n",
    "\n",
    "    print ('\\nLatitude = ',lat,'   Longitude = ', lon)\n",
    "    print ('Time to hv turn on/off = ',transition)\n",
    "\n",
    "    print('\\nHardness ratios --      HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "    print ('Solar RA, dec (radians): ', sun_ra_rad, sun_dec_rad)\n",
    "    print ('HXM RA, dec (radians)  :   ', hxm_ra_rad, hxm_dec_rad)\n",
    "    print ('SGM RA, dec (radians)  :', sgm_ra_rad, sgm_dec_rad)\n",
    "    print('HXM solar angle (theta, degrees) : %.3f' % (solar_hxm[0]))\n",
    "    print('SGM solar angle (theta, degrees) : %.3f' % (solar_sgm[0]))\n",
    "\n",
    "    print('\\nGOES average activity = ', GOES_avg, '     Variance of GOES activity = ', GOES_var) \n",
    "    print('GOES avg +/1 hr = ', GOES_avg_1hr,'      Variance +/- 1 hr = ',GOES_var_1hr)\n",
    "\n",
    "    print ('\\nTime history fit parameters (Gaussian):\\nAmplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,   FWHM = %5.1f,    chi-sq = %5.1f' \n",
    "        %(ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1) )\n",
    "    print ('Time history fit parameters (Norris):\\nAmplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,    t90 = %5.1f,    chi-sq = %5.1f' \n",
    "           % (ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2) )\n",
    "    print ('\\nLikelihood ratios:')\n",
    "    space = ' '\n",
    "    for idata in range (len(data_types)):\n",
    "        num_spaces = 21 - len (data_types [idata])\n",
    "        if idata < 5 or idata == 6 or idata == 7 or idata == 9:\n",
    "            print (num_spaces*space, data_types [idata],\":  {:9.2f}\".format(likelihood_list[idata]),\"     LGRB = {:9.2f}\".format(LGRB[idata]),\n",
    "                \"   Lsolar = {:9.2f}\".format(Lsolar[idata]), \"     Lpart = {:9.2f}\".format(Lpart [idata]))\n",
    "        else:\n",
    "            print (num_spaces*space, data_types [idata],\":  {:.3e}\".format(likelihood_list[idata]),\"     LGRB = {:9.2f}\".format(LGRB[idata]),\n",
    "                \"   Lsolar = {:9.2f}\".format(Lsolar[idata]), \"     Lpart = {:9.2f}\".format(Lpart [idata]))\n",
    "    print (18*space, 'Avg :             ', end = space)\n",
    "    for ievent in range (len(event_classes)):\n",
    "        print (\"          %9.2f\" % (sum_data_types [ievent]) , end = (ievent + 3) * space)\n",
    "    print ('\\n',14*space, 'Product :             ', end = space)\n",
    "    for ievent in range (len(event_classes)):\n",
    "        print (\"          %9.2f\" % (sum_log_data_types [ievent]) , end = (ievent + 3) * space)\n",
    "\n",
    "\n",
    "#    print ('\\n**************************')\n",
    "    \n",
    " ################################   \n",
    "# Determine previous classification from event_file (listing of previous events with classifications)\n",
    "    \"\"\"\n",
    "    classification = 'Unknown'\n",
    "    for i in range (len(event_id_rows)):\n",
    "#        print(trigid-int(event_id_rows[i]), event_id_rows[i], event_UT_rows[i], event_type_rows[i])   \n",
    "        if trigid == int(event_id_rows[i]):\n",
    "#            print (i, event_id_rows[i], event_UT_rows[i], event_type_rows[i])   \n",
    "            classification = event_type_rows[i]\n",
    "            break\n",
    "    \"\"\"     \n",
    "    classification = '????'   \n",
    "    for i in range (len(prior_trig_id)):\n",
    "#        print (trigid, type (trigid), prior_trig_id[i], type (prior_trig_id[i]))\n",
    "#        if str(trigid) == prior_trig_id[i]:\n",
    "        if abs(trigid - int(prior_trig_id[i])) < 2:   # Is trigid equal to or within 1 of prior_trig_id?\n",
    "            classification = prior_trig_class[i]\n",
    "            break\n",
    "    print ('\\n   Prior classification:    ',classification,'\\n')\n",
    "    \n",
    "\n",
    "################################    \n",
    "# Open and write to output summary file and values file\n",
    "\n",
    "#    print(' **** 11   Start on summary file, trig_num = ', trig_num, len(triggers))\n",
    "    if trig_num == 0:\n",
    "        global summary_file, summary_file_name\n",
    "        now = datetime.datetime.now()\n",
    "        d1 = now.strftime(\"%Y%m%d_%H%M\")\n",
    "        summary_file_name = \"Output_summary_file_\" + fileid + '_' + d1+'.txt'\n",
    "        summary_file = open (summary_file_name,\"w\")\n",
    "        values_file = open (\"values_file.txt\",\"a\")\n",
    "        values_file.write (d1 + '   ' + UT + '   ' + str(trigid) + '\\n')\n",
    "#        values_file = open (\"values_file.txt\",\"a\")\n",
    "    \n",
    "        line = 0\n",
    "        while line < len(data_types): \n",
    "            values_file.write (data_types[line] + ' ')\n",
    "            line += 1\n",
    "        values_file.write ('\\n')\n",
    "        summary_file.write (summary_file_name + '   written by   ' + os.path.basename(sys.argv[0]) + '   ' + str(now) + '\\n\\n') \n",
    "    if trig_num > 0:\n",
    "        summary_file = open (summary_file_name,\"a\")   \n",
    "        values_file = open (\"values_file.txt\",\"a\")\n",
    "\n",
    "    summary_file.write ('#%d of %d trigger(s) in %s \\n' % (trig_num + 1, len(triggers), fileid))   # print number of triggers\n",
    "    summary_file.write ('   TriggerID: %d \\n' % (trigid))\n",
    "    summary_file.write ('   Trigger Time (MDC): %.6f ' % (trigtime_mdc))\n",
    "    summary_file.write ('   Trigger Time (MET): %.6f ' % (trigtime_met))\n",
    "    summary_file.write ('   Trigger Time (UT): %s \\n\\n' % (UT))\n",
    "\n",
    "    summary_file.write ('   Latitude = %5.1f   Longitude = %5.1f \\n' % (lat, lon))\n",
    "    summary_file.write ('   Time to hv turn on/off = %d \\n\\n' % transition)\n",
    "\n",
    "    summary_file.write ('   Hardness ratios -- HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n\\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "    summary_file.write ('   Solar RA, dec (radians): %7.3f   %7.3f \\n' % (sun_ra_rad, sun_dec_rad))\n",
    "    summary_file.write ('   HXM RA, dec (radians):   %7.3f   %7.3f   ' % (hxm_ra_rad, hxm_dec_rad))\n",
    "    summary_file.write ('   SGM RA, dec (radians): %7.3f   %7.3f \\n' % (sgm_ra_rad, sgm_dec_rad))\n",
    "    summary_file.write ('   HXM solar angle (theta, degrees) = %7.3f   ' % (solar_hxm[0]))\n",
    "    summary_file.write ('   SGM solar angle (theta, degrees) = %7.3f \\n\\n' % (solar_sgm[0]))\n",
    "\n",
    "    summary_file.write ('\\nGOES average activity %10.3f   Variance of GOES activity = %10.3f' % (GOES_avg, GOES_var) )\n",
    "    summary_file.write ('\\nGOES avg act +/- 1 hr %10.3f   Variance of GOES +/- 1 hr = %10.3f' % (GOES_avg_1hr, GOES_var_1hr) )\n",
    "\n",
    "    summary_file.write ('   Time history fit parameters (Gaussian):\\n      Amplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,   FWHM = %5.1f,    chi-sq = %5.1f \\n' % (ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1) )\n",
    "    summary_file.write ('   Time history fit parameters (Norris):\\n      Amplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,    t90 = %5.1f,    chi-sq = %5.1f \\n\\n' % (ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2) )\n",
    "\n",
    "    for idata in range (len(data_types)):\n",
    "        summary_file.write (data_types [idata])\n",
    "        num_spaces = 21 - len(data_types[idata])\n",
    "\n",
    "        for j in range (num_spaces):\n",
    "            summary_file.write (\" \")\n",
    "        if likelihood_list[idata] >= 100000 or likelihood_list[idata] < .01:\n",
    "            summary_file.write (\"%.3e\" % (likelihood_list[idata]))\n",
    "        else:\n",
    "            summary_file.write (\"%9.2f\" % (likelihood_list[idata]))\n",
    "        summary_file.write (\"        LGRB = %9.2f   Lsolar = %9.2f    Lpart = %9.2f \\n\" % (LGRB[idata], Lsolar [idata], Lpart [idata]) )\n",
    "\n",
    "        if idata > 9:\n",
    "            if idata == 10 or idata == 12:\n",
    "                likelihood_list[idata] = float ( likelihood_list[idata]) * 1.0e8\n",
    "            if idata == 11 or idata == 13:\n",
    "                likelihood_list[idata] = float ( likelihood_list[idata]) * 1.0e12\n",
    "#            values_file.write (str(likelihood_list[idata]))\n",
    "#            values_file.write ('      ')\n",
    "#        else:\n",
    "#            values_file.write (str(int(likelihood_list[idata]*1000)/1000) + '      ')\n",
    "        values_file.write (str(int(likelihood_list[idata]*1000)/1000) + '      ')\n",
    "\n",
    "    summary_file.write ('            Avg :                ')\n",
    "    for ievent in range (len(event_classes)):                \n",
    "        summary_file.write (\"            %9.2f\" % (sum_data_types [ievent]))\n",
    "    summary_file.write ('\\nPrior classification:     ' + classification + '\\n\\n**************************\\n\\n')\n",
    "    values_file.write ('Prev. class = ' + classification + '\\n')\n",
    "\n",
    "# Combine individual plots into single file\n",
    "#    pdfs = ['plot1.pdf', 'plot2.pdf', 'plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', 'plot4.pdf']\n",
    "# Why is plot1 redundant?\n",
    "    if GOES_avg == -1:\n",
    "        pdfs = ['Plot2.pdf','plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', \n",
    "        'plot4.pdf'] \n",
    "    else:\n",
    "       pdfs = ['Plot2.pdf','plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', \n",
    "        'plot4.pdf', 'plot_solar.pdf', 'GOES_flux.pdf']\n",
    "#    merger = PdfMerger()\n",
    "    merger = PdfWriter()\n",
    "    for pdf in pdfs:\n",
    "        merger.append (pdf)\n",
    "    filename = \"summary_plot_file_\" + fileid + '-'+str(trig_num)+'.pdf'\n",
    "\n",
    "    merger.write (filename)\n",
    "\n",
    "    merger.close()\n",
    "    plt.close()\n",
    "\n",
    "# Delete plot files after they are merged together.\n",
    "    os.remove (\"Plot2.pdf\")\n",
    "    os.remove (\"plot31.pdf\")\n",
    "    os.remove (\"plot32.pdf\")\n",
    "    os.remove (\"plot33.pdf\")\n",
    "    os.remove (\"plot34.pdf\")\n",
    "    os.remove (\"plot35.pdf\")\n",
    "    os.remove (\"plot36.pdf\")\n",
    "    os.remove (\"plot4.pdf\")\n",
    "    if trig_num == len(triggers) - 1 :   \t# After final trigger, close plots of geomagnetic activity\n",
    "        os.remove (\"plot_solar.pdf\")\n",
    "        if GOES_avg > -1:\n",
    "            os.remove (\"GOES_flux.pdf\")\n",
    "\n",
    "# After each trigger, close summary file, close values file\n",
    "#    if trig_num == len(triggers) - 1 :\n",
    "    summary_file.close()\n",
    "    values_file.close()\n",
    "#    print(' **** 12   Return from writing, trig_num = ', trig_num, len(triggers))\n",
    "    return\n",
    "\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main(fileid, chan_type, tmin, tmax)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "def analyze_single_trigger (fileid, chan_type, data_dir, ph_table, trig_index, trig_time, trig_num, triggers,\n",
    "event_id_rows, event_UT_rows, event_type_rows, tmin, tmax):\n",
    "#    This is basically analyze8.py\n",
    "    trigtime_met = ref_MET = trig_time[trig_num]  #  Multiple triggers?   # These three lines ensure compatibility with process_trigger\n",
    "    trigtime_mdc = trig_index[trig_num]\n",
    "    trigid = int(trigtime_mdc)\n",
    "\n",
    "    ph_table['ZTIME'] = ph_table['TIME'] - cg.PH_HBIN - ref_MET\n",
    "\n",
    "################################\n",
    "# Find nearest hv turn on/off time\n",
    "# Revised version -- Search over +-10800 s = 2700 time bins -- 6/17/24\n",
    "    \"\"\"\n",
    "    threshold = 2000\n",
    "    ir_on = ir_off = transition = turn_off = turn_on = -86400\n",
    "\n",
    "    ir_end = len(ph_table['ZTIME'])\n",
    "    for ir in range (1,ir_end):\n",
    "        if ph_table['ZTIME'][ir] > -4000:\t\t\t# Start search for transition at 4000 before trigger\n",
    "            ir_start = ir + 1\n",
    "            break\n",
    "    for ir in range (ir_start,min(ir_start+2000,ir_end)):\n",
    "        print (i2, ph_table['ZTIME'][ir])\n",
    "#        xx1 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "#        xx2 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir-1])\n",
    "        xx1 = int(np.sum(ph_table['SGML_COUNTS'], axis=1)[ir])\n",
    "        xx2 = int(np.sum(ph_table['SGML_COUNTS'], axis=1)[ir-1])\n",
    "\n",
    "        if (xx1-xx2) > threshold:\t\t\t\t# Starting at ir = -2000, is difference between two successive records > 2000?\n",
    "            turn_on = ph_table['ZTIME'][ir]\n",
    "            ir_on = ir\t\t\t\t\t\t# Identify index ir at which turn_on occurs\n",
    "        if (xx2-xx1) > threshold:\n",
    "            turn_off = ph_table['ZTIME'][ir]\n",
    "            ir_off = ir\t\t\t\t\t\t# Identify index ir at which turn_off occurs\n",
    "        if turn_on > -86400 and turn_off > -86400 :\t\t# Have I found both an up and a down transition within 2000 counts of trigger?\n",
    "            break\n",
    "    if abs(turn_on) > abs(turn_off):\n",
    "        transition = int(turn_off)\t\t\t\t# Transition = time of turn on/off, ir_off/ir_on = index\n",
    "        ir = ir_off\t\t\t\t\t\t#    - ==> turn on, + ==> turn off\n",
    "    else:\t\t\t\t\t\t\t#    -86400 means turn on at very start of event or never found a transition\n",
    "        transition = int(turn_on)\n",
    "        ir = ir_on\n",
    "    \"\"\"\n",
    "    threshold = 1000\n",
    "    imax= len(ph_table['ZTIME'])\n",
    "#    print ('Start time: ', ph_table['ZTIME'][0], '  End time: ',ph_table['ZTIME'] [imax-1] )\n",
    "    transition = 86400\n",
    "    ir_transition = 0\n",
    "    ir_start = 0\n",
    "    ir_end = len(ph_table['ZTIME'])\n",
    "#    for ir in range (ir_start, ir_end,5):\n",
    "#        print ('@@@@',ir, ph_table['ZTIME'][ir], np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "    ir_start = -float(ph_table['ZTIME'][0])/4 - 2700\t\t# Start 1/2 orbit prior to trigger, extend to 1/2 orbit after\n",
    "    ir_start = int(ir_start)\n",
    "    ir_end = ir_start + 5400\n",
    "    for ir in range (ir_start, ir_end):\n",
    "        xx1 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir])\n",
    "        xx2 = int(np.sum(ph_table['HXM1L_COUNTS'], axis=1)[ir-1])\n",
    "        if abs(xx1-xx2) > threshold:\t\t\t\t# Starting at ir = -2700, is difference between two successive records > 2000?\n",
    "            turn_on = int(ph_table['ZTIME'][ir])\t\t# Tentative time at which transition occurs\n",
    "            if abs(turn_on) < abs(transition):\n",
    "                transition = turn_on\t\t\t\t# Identify time at which closest turn_on/off occurs\n",
    "                ir_transition = ir\t\t\t\t# Identify index at which closest turn_on/off occurs\n",
    "\n",
    "    print ('Time to turn on/off (sec) = ',transition)\n",
    "\n",
    "#   Print counts vs time around hv transition\n",
    "    ir_first = ir_transition - 10\n",
    "    if (ir_first < ir_start):\n",
    "        ir_first = ir_start\n",
    "    ir_last = ir_first + 20\n",
    "    if (ir_last > ir_end):\n",
    "        ir_last = ir_end\n",
    "    if ir_first <= ir_last :\n",
    "        print ('Counts around hv turn on/off transition:')\n",
    "        for i in range (ir_first, ir_last):\n",
    "            print(' HXM1L counts near hv transition: ', ph_table['ZTIME'][i],np.sum(ph_table['HXM1L_COUNTS'], axis = 1)[i])\n",
    "        print ()\n",
    "#    print ('*** Finished 1')\n",
    "\n",
    "################################\n",
    "# Plot distribution of counts vs time \n",
    "    time_dist(ph_table['ZTIME'], ph_table['HXM1L_COUNTS'], ph_table['HXM2L_COUNTS'], ph_table['SGML_COUNTS'], ref_MET, fileid, trig_num)\n",
    "   \n",
    "################################\n",
    "# Fit counts vs time plus background -- Gaussian\n",
    "    ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1, d, e, f = fitter(ph_table['ZTIME'], ph_table['SGML_COUNTS'], ref_MET, tmin, tmax)\n",
    "# Fit counts vs time plus background -- Norris function\n",
    "\n",
    "    ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2 = fitter_Norris(ph_table['ZTIME'], \n",
    "        ph_table['SGML_COUNTS'], ref_MET, peak_pos1, FWHM, d, e, f)\n",
    "#    print ('***** 2 Return from Norris fit')\n",
    "    \n",
    "################################\n",
    "# Plot spectra vs energy\n",
    "#   Use default start and stop time\n",
    "\n",
    "#    tmin = float(input(\"Please input the start time:\"))\n",
    "#    tmax = float(input(\"Please input the end time:\"))\n",
    "#    print ()\n",
    "#    tmin = -200\n",
    "#    tmax = 200\n",
    "    tmask = (tmin <= ph_table['ZTIME']) & (ph_table['ZTIME'] <= tmax)\n",
    "    roi = ph_table[tmask]\n",
    "    row_num = float(len(roi))\n",
    "\n",
    "    detectors = ['HXM1H', 'HXM2H', 'SGMH', 'HXM1L', 'HXM2L', 'SGML']\n",
    "    id = 0  \n",
    "    sumplot = np.zeros (len(detectors))\n",
    "    for detector in detectors:\n",
    "        y = np.sum(roi[detector + '_RATE'], axis = 0)\n",
    "        if (detector == 'HXM1H' or detector == 'HXM2H' or detector == 'SGMH'):      # Change x, z values for first 3 high-threshold detectors\n",
    "            Emin = 20\n",
    "            Emax = 100\n",
    "            x = np.arange(cg.PH_BINNUM_HIGH)\n",
    "            fig = plt.figure(figsize=(8, 6))\n",
    "            plt.plot( x, y / cg.high_ph_width() / row_num, drawstyle='steps-mid')\n",
    "        else:                                                                       # Change x, z values for last 3 low-threshold detectors\n",
    "            Emin = 10\n",
    "            Emax = 400\n",
    "            x = np.arange(cg.PH_BINNUM_LOW)\n",
    "            fig = plt.figure(figsize=(8, 6))\n",
    "            plt.plot( x, y / cg.low_ph_width() / row_num, drawstyle='steps-mid')\n",
    "\n",
    "#    Calculate hardness ratio between Emin and Emax separately for low- and high-threshold detectors\n",
    "        for i in range (0, len(x)):        # Sum up counts in appropriate energy ranges\n",
    "            if (x[i] > Emin) & (x[i] < Emax):\n",
    "                sumplot[id] += y[i]\n",
    "   \n",
    "        id += 1\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('PH channel')\n",
    "        plt.ylabel('Counts /s /ADC channel')\n",
    "        plt.title(detector + ' spectrum')\n",
    "        fig.savefig ('plot3'+str(id)+'.pdf')\n",
    "#       plt.show()\n",
    "        plt.close (fig)\n",
    "\n",
    "#    print ('**** 3 Return from plotting spectra vs energy')\n",
    "\n",
    "################################\n",
    "#     Hardness ratios\n",
    "    HXM1_ratio = sumplot[3]/sumplot[0]    \n",
    "    HXM2_ratio = sumplot[4]/sumplot[1]\n",
    "    SGM_ratio  = sumplot[5]/sumplot[2]\n",
    "#    print('Hardness ratios --      HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "#    print ('****   4  Return from hardness ratio calculation')\n",
    "\n",
    "################################\n",
    "#     determine latitude\n",
    "#    plot_orb(data_dir, fileid, ref_MET)     # Multiple trigger times?\n",
    "\n",
    "################################\n",
    "#    determine latitude and angle to Sun\n",
    "    sun_ra_rad, sun_dec_rad, hxm_ra_deg, hxm_dec_deg, sgm_ra_deg, sgm_dec_deg, solar_hxm, solar_sgm, lat, lon, UT = process_trigger(\n",
    "          data_dir, fileid, trigid, trigtime_met, trigtime_mdc)\n",
    "    if (sun_dec_rad) < 0 :\n",
    "        sun_dec_rad += 2.*math.pi\n",
    "\n",
    "    hxm_ra_rad = hxm_ra_deg * math.pi/180.\n",
    "    hxm_dec_rad = hxm_dec_deg * math.pi/180.\n",
    "    sgm_ra_rad = sgm_ra_deg * math.pi/180.\n",
    "    sgm_dec_rad = sgm_dec_deg * math.pi/180.\n",
    "\n",
    "    alpha1 = sun_ra_rad\n",
    "    alpha2 = hxm_ra_rad\n",
    "    alpha3 = sgm_ra_rad\n",
    "    dec1 = sun_dec_rad\n",
    "    dec2 = hxm_dec_rad\n",
    "    dec3 = sgm_dec_rad\n",
    "\n",
    "# Calculation between 2 points with RA, dec using Haversine formula\n",
    "    theta12 = Haversine (dec1, dec2, alpha1, alpha2) * 180./math.pi\n",
    "    theta13 = Haversine (dec1, dec3, alpha1, alpha3) * 180./math.pi\n",
    "\n",
    "# Calculation between 2 points with theta, phi using dot product of vectors\n",
    "    theta1 = phi1 = 0.\n",
    "    theta2 = solar_hxm[0] * math.pi/180.\n",
    "    phi2 = solar_hxm[1] * math.pi/180.\n",
    "    delta_theta_hxm = math.acos( math.sin(theta1) * math.cos(phi1) * math.sin(theta2) * math.cos(phi2) \n",
    "                          + math.sin(theta1) * math.sin(phi1) * math.sin(theta2) * math.sin(phi2)\n",
    "                          + math.cos(theta1) * math.cos(theta2) ) * 180./math.pi\n",
    "    theta3 = solar_sgm[0] * math.pi/180.\n",
    "    phi3 = solar_sgm[1] * math.pi/180.\n",
    "    delta_theta_sgm = math.acos( math.sin(theta1) * math.cos(phi1) * math.sin(theta3) * math.cos(phi3) \n",
    "                          + math.sin(theta1) * math.sin(phi1) * math.sin(theta3) * math.sin(phi3)\n",
    "                          + math.cos(theta1) * math.cos(theta3) ) * 180./math.pi\n",
    "\n",
    "################################\n",
    "#     Solar activity\n",
    "\n",
    "    global GOES_avg, GOES_var\n",
    "    duration = \"1200\"\n",
    "    GOES_avg = GOES_var = -1\n",
    "#    print('*** 5 Enter solar', trig_num, GOES_avg, GOES_var)\n",
    "    if trig_num == 0 or trig_num > 0:\n",
    "        GOES_avg, GOES_var, GOES_avg_1hr, GOES_var_1hr = Xray_flux1 (fileid, UT)\n",
    "        try: \n",
    "            solar (fileid, UT, duration)\n",
    "        except:\n",
    "            duration = '1200'\n",
    "    print ('**** 6 Trig_num =', trig_num, ' Return from solar', GOES_avg, GOES_var, GOES_avg_1hr, GOES_var_1hr)\n",
    "################################\n",
    "\n",
    "    \"\"\"\n",
    "# As of 2/22/2004 -- Calculate likelihoods for -- old version\n",
    "#        lat, transition, angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0]), FWHM, t90\n",
    "# To add categories, add them to data_types list in main routing and likelihood_list here\n",
    "# data_types, x, Ngtx, event_classes are global variables\n",
    "# data_types = ['Latitude','Time_to_transition','angle_to_Sun','FWHM','t90']\t-- Copy of data_types from main routine\n",
    "# event_classes = ['GRB', 'Solar','Particle']\t\t\t\t-- Copy of event classes from main routine\n",
    "# Ngtx [itype][events][row_number] = int (Ngtx [itype][events][row_number]) / norm [events]  --  Copy from main routine\n",
    "\n",
    "    angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0])\n",
    "    likelihood_list = [lat, transition, angle_to_Sun, FWHM, ampl1, chisq1, t90, ampl2, chisq2, SGM_ratio, GOES_avg, GOES_var, \n",
    "        GOES_avg_1hr, GOES_var_1hr]\n",
    "\n",
    "    LGRB = [0] * len(data_types)\n",
    "    Lsolar =[0] * len(data_types)\n",
    "    Lpart = [0] * len(data_types)\n",
    "    for idata in range (len(data_types)):\n",
    "        for ievent in range (len(event_classes)):\n",
    "            total = 0\n",
    "            if ievent == 0:\n",
    "                LGRB [idata] = 0 \n",
    "            if ievent == 1:\n",
    "                Lsolar [idata] = 0\n",
    "            if ievent == 2:\n",
    "                Lpart [idata] = 0\n",
    "            jrow = 0\n",
    "            while (jrow < max_rows):\n",
    "                jrow += 1\n",
    "#                print ('lat = ', lat, likelihood_list [idata], type(likelihood_list [idata]))\n",
    "#                print ('Ngtx ', idata, ievent, jrow, Ngtx [idata][ievent][jrow], type (Ngtx [idata][ievent][jrow]))  \n",
    "#                print ('xL ', idata, ievent, jrow, xL [idata][ievent][jrow], type (xL [idata][ievent][jrow]))\n",
    "#                print ('@@@@@$$$$$  ', idata, ievent, jrow, xL [idata][ievent][jrow], Ngtx [idata][ievent][jrow],\n",
    "#                    type(xL [idata][ievent][jrow]), type(Ngtx [idata][ievent][jrow]) )\n",
    "                if likelihood_list [idata] < float(xL [idata][ievent][jrow]) or Ngtx [idata][ievent][jrow] == 0:\n",
    "                    jrow -= 1\n",
    "                    if ievent == 0:\n",
    "                        LGRB [idata] = Ngtx [idata][ievent][jrow] \n",
    "                    if ievent == 1:\n",
    "                        Lsolar [idata] = Ngtx [idata][ievent][jrow] \n",
    "                    if ievent == 2:\n",
    "                        Lpart [idata] = Ngtx [idata][ievent][jrow]  \n",
    "                    total += Ngtx [idata][ievent][jrow]  \n",
    "                    jrow = 10000 \n",
    "#            continue\n",
    "#        print ('***** 9c ', idata, ievent)\n",
    "        LGRB [idata] = LGRB [idata] / total\n",
    "        Lsolar [idata] = Lsolar [idata] / total\n",
    "        Lpart [idata] = Lpart [idata] / total    \n",
    "\n",
    "    sum_data_types = [0] * len(event_classes)\t\t\t\t\t\t# Calculate Sum (L) and Product (ln L)\n",
    "    sum_log_data_types = [1] * len(event_classes)\n",
    "    count_sum = [0] * len(event_classes)\n",
    "    for idata in range (len(data_types)):\n",
    "        sum_data_types [1] += Lsolar [idata]\n",
    "        sum_data_types [2] += Lpart [idata]\n",
    "        if LGRB [idata] > 0:\n",
    "            sum_data_types [0] += LGRB [idata]\n",
    "            sum_log_data_types [0] = sum_log_data_types [0] + math.log (LGRB[idata])\n",
    "            count_sum [0] += 1\n",
    "         if Lsolar [idata] > 0:\n",
    "            sum_data_types [1] += Lsolar [idata]\n",
    "            sum_log_data_types [1] = sum_log_data_types [1] + math.log (Lsolar[idata])\n",
    "            count_sum [1] += 1\n",
    "         if Lpart [idata] > 0:\n",
    "            sum_data_types [2] += Lpart [idata]\n",
    "            sum_log_data_types [2] = sum_log_data_types [2] + math.log (Lpart[idata])\n",
    "            count_sum [02 += 1\n",
    "     \n",
    "    for ievent in range (len(event_classes)):\t\t\t\t\t\t# Normalize by number of non-zero likelihoods\n",
    "#        sum_data_types [ievent] = sum_data_types [ievent] / len (data_types)              \n",
    "        sum_data_types [ievent] = sum_data_types [ievent] / count_sum [ievent]\n",
    "        sum_log_data_types [ievent] = - math.exp (sum_log_data_types [ievent] ** (1 / count_sum [ievent] ))\n",
    "\n",
    "#    print ('*** 10  End likelihoods, enter print section, trig_num = ', trig_num)\n",
    "    \"\"\"\n",
    "\n",
    "##################################################################    \n",
    "\n",
    "# As of 6/5/2024 -- Calculate likelihoods for -- New version\n",
    "#        lat, transition, angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0]), FWHM, t90\n",
    "# To add categories, add them to data_types list in main routing and likelihood_list here\n",
    "# data_types, x, Ngtx, event_classes are global variables\n",
    "# data_types = ['Latitude','Time_to_transition','angle_to_Sun','FWHM','t90']\t-- Copy of data_types from main routine\n",
    "# event_classes = ['GRB', 'Solar','Particle']\t\t\t\t-- Copy of event classes from main routine\n",
    "\n",
    "    angle_to_Sun = .5 *(solar_hxm[0] + solar_sgm [0])\n",
    "    likelihood_list = [lat, transition, angle_to_Sun, FWHM, ampl1, chisq1, t90, ampl2, chisq2, SGM_ratio, GOES_avg, GOES_var,\n",
    "        GOES_avg_1hr, GOES_var_1hr]\n",
    "    L_ratio = [0] * len(event_classes)\n",
    "\n",
    "    LGRB = [0] * len(data_types)\n",
    "    Lsolar =[0] * len(data_types)\n",
    "    Lpart = [0] * len(data_types)\n",
    "#    for idata in range (len(data_types)):\n",
    "    for i, category in data_types:\n",
    "        x = float( likelihood_list [i])\n",
    "        if idata == 2: \t\t\t\t# Normalizations\n",
    "            arg = math.sin (x *3.14159265/180.)\n",
    "            x = arg * arg\n",
    "        if idata == 10:\t\t\t\t\n",
    "            x = x * 1.0e8\n",
    "        if idata == 11:\t\t\t\t\n",
    "            x = x * 1.0e12\n",
    "        \n",
    "        total = 0\n",
    "        for iclass in range (len(event_classes)):\n",
    "            for bin in range (1, max_bins):\n",
    "                L_ratio [iclass] = 0\n",
    "#                if likelihood_list [idata] > float(midpoints [idata] [max_bins - 1]):\n",
    "                if x > float(midpoints [i] [max_bins - 1]):\n",
    "                    break\n",
    "#                if likelihood_list [idata] <= float(midpoints [idata] [bin]):\n",
    "                if x <= float(midpoints [is] [bin]):\n",
    "#                    x = float( likelihood_list [idata])\n",
    "                    x2 =float( midpoints [idata] [bin])\n",
    "                    x1 = float( midpoints [idata] [bin-1])\n",
    "                    if x < x1:\n",
    "                        break\n",
    "                    y2 = float( likelihood [idata] [iclass] [bin] )   \n",
    "                    y1 = float( likelihood [idata] [iclass] [bin-1])  \n",
    "                    L_ratio [iclass] = (y2 - y1) / (x2 - x1) * (x - x1) + y1\n",
    "#                    if idata == 2:\n",
    "#                        print (x, x1, x, x2, y1, y2, L_ratio [iclass])\n",
    "                    break\n",
    "            total += L_ratio [iclass]\n",
    "        for iclass in range (len(event_classes)):\n",
    "            if total != 0:\n",
    "                L_ratio [iclass] = L_ratio [iclass] / total    # For each event, L_ratio is GRB/solar/particle likelihood for particular data class\n",
    "#        print (total, L_ratio [0], L_ratio [1], L_ratio [2])\n",
    "        LGRB [idata] = L_ratio [0] \n",
    "        Lsolar [idata] = L_ratio [1]\n",
    "        Lpart [idata] = L_ratio [2] \n",
    "#        sys.exit()\n",
    "#        print ('@@@@@',LGRB[idata], Lsolar[idata], Lpart[idata], total)\n",
    "\n",
    "    sum_data_types = [0] * len(event_classes)\t\t\t\t\t\t# Calculate Sum (L) and Product (ln L)\n",
    "    sum_log_data_types = [1] * len(event_classes)\n",
    "    count_sum = [0] * len(event_classes)\n",
    "    for idata in range (len(data_types)):\n",
    "        sum_data_types [1] += Lsolar [idata]\n",
    "        sum_data_types [2] += Lpart [idata]\n",
    "        if LGRB [idata] > 0:\n",
    "            sum_data_types [0] += LGRB [idata]\n",
    "            sum_log_data_types [0] = sum_log_data_types [0] + math.log (LGRB[idata])\n",
    "            count_sum [0] += 1\n",
    "        if Lsolar [idata] > 0:\n",
    "            sum_data_types [1] += Lsolar [idata]\n",
    "            sum_log_data_types [1] = sum_log_data_types [1] + math.log (Lsolar[idata])\n",
    "            count_sum [1] += 1\n",
    "        if Lpart [idata] > 0:\n",
    "            sum_data_types [2] += Lpart [idata]\n",
    "            sum_log_data_types [2] = sum_log_data_types [2] + math.log (Lpart[idata])\n",
    "            count_sum [2] += 1\n",
    "     \n",
    "    for ievent in range (len(event_classes)):\t\t\t\t\t\t# Normalize by number of non-zero likelihoods\n",
    "#        sum_data_types [ievent] = sum_data_types [ievent] / len (data_types)              \n",
    "        sum_data_types [ievent] = sum_data_types [ievent] / count_sum [ievent]\n",
    "        sum_log_data_types [ievent] = math.exp (sum_log_data_types [ievent]) ** (1 / count_sum [ievent] )\n",
    "\n",
    "################################\n",
    "#     Print out results\n",
    "\n",
    "#    if trig_num > 0:\n",
    "#        summary_file.write (summary_file_name + '   written by   ' + os.path.basename(sys.argv[0]) + '   ' + str(now) + '\\n\\n') \n",
    "    print('\\n %d of %d trigger(s) in %s ' % (trig_num + 1, len(triggers), fileid), end = '   --   ')   # print number of triggers\n",
    "\n",
    "    print('TriggerID: %d     \\nTrigger Time (MDC): %.6f     Trigger Time (MET): %.6f     Trigger Time (UT): %s' \n",
    "          % (trigid, trigtime_mdc, trigtime_met, UT))\n",
    "\n",
    "    print ('\\nLatitude = ',lat,'   Longitude = ', lon)\n",
    "    print ('Time to hv turn on/off = ',transition)\n",
    "\n",
    "    print('\\nHardness ratios --      HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "    print ('Solar RA, dec (radians): ', sun_ra_rad, sun_dec_rad)\n",
    "    print ('HXM RA, dec (radians)  :   ', hxm_ra_rad, hxm_dec_rad)\n",
    "    print ('SGM RA, dec (radians)  :', sgm_ra_rad, sgm_dec_rad)\n",
    "    print('HXM solar angle (theta, degrees) : %.3f' % (solar_hxm[0]))\n",
    "    print('SGM solar angle (theta, degrees) : %.3f' % (solar_sgm[0]))\n",
    "\n",
    "    print('\\nGOES average activity = ', GOES_avg, '     Variance of GOES activity = ', GOES_var) \n",
    "    print('GOES avg +/1 hr = ', GOES_avg_1hr,'      Variance +/- 1 hr = ',GOES_var_1hr)\n",
    "\n",
    "    print ('\\nTime history fit parameters (Gaussian):\\nAmplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,   FWHM = %5.1f,    chi-sq = %5.1f' \n",
    "        %(ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1) )\n",
    "    print ('Time history fit parameters (Norris):\\nAmplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,    t90 = %5.1f,    chi-sq = %5.1f' \n",
    "           % (ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2) )\n",
    "    print ('\\nLikelihood ratios:')\n",
    "    space = ' '\n",
    "    for idata in range (len(data_types)):\n",
    "        num_spaces = 21 - len (data_types [idata])\n",
    "        if idata < 5 or idata == 6 or idata == 7 or idata == 9:\n",
    "            print (num_spaces*space, data_types [idata],\":  {:9.2f}\".format(likelihood_list[idata]),\"     LGRB = {:9.2f}\".format(LGRB[idata]),\n",
    "                \"   Lsolar = {:9.2f}\".format(Lsolar[idata]), \"     Lpart = {:9.2f}\".format(Lpart [idata]))\n",
    "        else:\n",
    "            print (num_spaces*space, data_types [idata],\":  {:.3e}\".format(likelihood_list[idata]),\"     LGRB = {:9.2f}\".format(LGRB[idata]),\n",
    "                \"   Lsolar = {:9.2f}\".format(Lsolar[idata]), \"     Lpart = {:9.2f}\".format(Lpart [idata]))\n",
    "    print (18*space, 'Avg :             ', end = space)\n",
    "    for ievent in range (len(event_classes)):\n",
    "        print (\"          %9.2f\" % (sum_data_types [ievent]) , end = (ievent + 3) * space)\n",
    "    print ('\\n',14*space, 'Product :             ', end = space)\n",
    "    for ievent in range (len(event_classes)):\n",
    "        print (\"          %9.2f\" % (sum_log_data_types [ievent]) , end = (ievent + 3) * space)\n",
    "\n",
    "\n",
    "#    print ('\\n**************************')\n",
    "    \n",
    " ################################   \n",
    "# Determine previous classification from event_file (listing of previous events with classifications)\n",
    "    \"\"\"\n",
    "    classification = 'Unknown'\n",
    "    for i in range (len(event_id_rows)):\n",
    "#        print(trigid-int(event_id_rows[i]), event_id_rows[i], event_UT_rows[i], event_type_rows[i])   \n",
    "        if trigid == int(event_id_rows[i]):\n",
    "#            print (i, event_id_rows[i], event_UT_rows[i], event_type_rows[i])   \n",
    "            classification = event_type_rows[i]\n",
    "            break\n",
    "    \"\"\"     \n",
    "    classification = '????'   \n",
    "    for i in range (len(prior_trig_id)):\n",
    "#        print (trigid, type (trigid), prior_trig_id[i], type (prior_trig_id[i]))\n",
    "#        if str(trigid) == prior_trig_id[i]:\n",
    "        if abs(trigid - int(prior_trig_id[i])) < 2:   # Is trigid equal to or within 1 of prior_trig_id?\n",
    "            classification = prior_trig_class[i]\n",
    "            break\n",
    "    print ('\\n   Prior classification:    ',classification,'\\n')\n",
    "    \n",
    "\n",
    "################################    \n",
    "# Open and write to output summary file and values file\n",
    "\n",
    "#    print(' **** 11   Start on summary file, trig_num = ', trig_num, len(triggers))\n",
    "    if trig_num == 0:\n",
    "        global summary_file, summary_file_name\n",
    "        now = datetime.datetime.now()\n",
    "        d1 = now.strftime(\"%Y%m%d_%H%M\")\n",
    "        summary_file_name = \"Output_summary_file_\" + fileid + '_' + d1+'.txt'\n",
    "        summary_file = open (summary_file_name,\"w\")\n",
    "        values_file = open (\"values_file.txt\",\"a\")\n",
    "        values_file.write (d1 + '   ' + UT + '   ' + str(trigid) + '\\n')\n",
    "#        values_file = open (\"values_file.txt\",\"a\")\n",
    "    \n",
    "        line = 0\n",
    "        while line < len(data_types): \n",
    "            values_file.write (data_types[line] + ' ')\n",
    "            line += 1\n",
    "        values_file.write ('\\n')\n",
    "        summary_file.write (summary_file_name + '   written by   ' + os.path.basename(sys.argv[0]) + '   ' + str(now) + '\\n\\n') \n",
    "    if trig_num > 0:\n",
    "        summary_file = open (summary_file_name,\"a\")   \n",
    "        values_file = open (\"values_file.txt\",\"a\")\n",
    "\n",
    "    summary_file.write ('#%d of %d trigger(s) in %s \\n' % (trig_num + 1, len(triggers), fileid))   # print number of triggers\n",
    "    summary_file.write ('   TriggerID: %d \\n' % (trigid))\n",
    "    summary_file.write ('   Trigger Time (MDC): %.6f ' % (trigtime_mdc))\n",
    "    summary_file.write ('   Trigger Time (MET): %.6f ' % (trigtime_met))\n",
    "    summary_file.write ('   Trigger Time (UT): %s \\n\\n' % (UT))\n",
    "\n",
    "    summary_file.write ('   Latitude = %5.1f   Longitude = %5.1f \\n' % (lat, lon))\n",
    "    summary_file.write ('   Time to hv turn on/off = %d \\n\\n' % transition)\n",
    "\n",
    "    summary_file.write ('   Hardness ratios -- HXM1: %5.1f     HXM2: %5.1f     SGM: %5.1f \\n\\n' % (HXM1_ratio, HXM2_ratio, SGM_ratio))\n",
    "    summary_file.write ('   Solar RA, dec (radians): %7.3f   %7.3f \\n' % (sun_ra_rad, sun_dec_rad))\n",
    "    summary_file.write ('   HXM RA, dec (radians):   %7.3f   %7.3f   ' % (hxm_ra_rad, hxm_dec_rad))\n",
    "    summary_file.write ('   SGM RA, dec (radians): %7.3f   %7.3f \\n' % (sgm_ra_rad, sgm_dec_rad))\n",
    "    summary_file.write ('   HXM solar angle (theta, degrees) = %7.3f   ' % (solar_hxm[0]))\n",
    "    summary_file.write ('   SGM solar angle (theta, degrees) = %7.3f \\n\\n' % (solar_sgm[0]))\n",
    "\n",
    "    summary_file.write ('\\nGOES average activity %10.3f   Variance of GOES activity = %10.3f' % (GOES_avg, GOES_var) )\n",
    "    summary_file.write ('\\nGOES avg act +/- 1 hr %10.3f   Variance of GOES +/- 1 hr = %10.3f' % (GOES_avg_1hr, GOES_var_1hr) )\n",
    "\n",
    "    summary_file.write ('   Time history fit parameters (Gaussian):\\n      Amplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,   FWHM = %5.1f,    chi-sq = %5.1f \\n' % (ampl1, peak_pos1, bkg1, s_over_rootB1, FWHM, chisq1) )\n",
    "    summary_file.write ('   Time history fit parameters (Norris):\\n      Amplitude = %8d,  Peak pos = %6d,   Bkg = %8d,  S/root(B) = %5.1f,    t90 = %5.1f,    chi-sq = %5.1f \\n\\n' % (ampl2, peak_pos2, bkg2, s_over_rootB2, t90, chisq2) )\n",
    "\n",
    "    for idata in range (len(data_types)):\n",
    "        summary_file.write (data_types [idata])\n",
    "        num_spaces = 21 - len(data_types[idata])\n",
    "\n",
    "        for j in range (num_spaces):\n",
    "            summary_file.write (\" \")\n",
    "        if likelihood_list[idata] >= 100000 or likelihood_list[idata] < .01:\n",
    "            summary_file.write (\"%.3e\" % (likelihood_list[idata]))\n",
    "        else:\n",
    "            summary_file.write (\"%9.2f\" % (likelihood_list[idata]))\n",
    "        summary_file.write (\"        LGRB = %9.2f   Lsolar = %9.2f    Lpart = %9.2f \\n\" % (LGRB[idata], Lsolar [idata], Lpart [idata]) )\n",
    "\n",
    "        if idata > 9:\n",
    "            if idata == 10 or idata == 12:\n",
    "                likelihood_list[idata] = float ( likelihood_list[idata]) * 1.0e8\n",
    "            if idata == 11 or idata == 13:\n",
    "                likelihood_list[idata] = float ( likelihood_list[idata]) * 1.0e12\n",
    "#            values_file.write (str(likelihood_list[idata]))\n",
    "#            values_file.write ('      ')\n",
    "#        else:\n",
    "#            values_file.write (str(int(likelihood_list[idata]*1000)/1000) + '      ')\n",
    "        values_file.write (str(int(likelihood_list[idata]*1000)/1000) + '      ')\n",
    "\n",
    "    summary_file.write ('            Avg :                ')\n",
    "    for ievent in range (len(event_classes)):                \n",
    "        summary_file.write (\"            %9.2f\" % (sum_data_types [ievent]))\n",
    "    summary_file.write ('\\nPrior classification:     ' + classification + '\\n\\n**************************\\n\\n')\n",
    "    values_file.write ('Prev. class = ' + classification + '\\n')\n",
    "\n",
    "# Combine individual plots into single file\n",
    "#    pdfs = ['plot1.pdf', 'plot2.pdf', 'plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', 'plot4.pdf']\n",
    "# Why is plot1 redundant?\n",
    "    if GOES_avg == -1:\n",
    "        pdfs = ['Plot2.pdf','plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', \n",
    "        'plot4.pdf'] \n",
    "    else:\n",
    "       pdfs = ['Plot2.pdf','plot31.pdf', 'plot32.pdf', 'plot33.pdf', 'plot34.pdf', 'plot35.pdf', 'plot36.pdf', \n",
    "        'plot4.pdf', 'plot_solar.pdf', 'GOES_flux.pdf']\n",
    "#    merger = PdfMerger()\n",
    "    merger = PdfWriter()\n",
    "    for pdf in pdfs:\n",
    "        merger.append (pdf)\n",
    "    filename = \"summary_plot_file_\" + fileid + '-'+str(trig_num)+'.pdf'\n",
    "\n",
    "    merger.write (filename)\n",
    "\n",
    "    merger.close()\n",
    "    plt.close()\n",
    "\n",
    "# Delete plot files after they are merged together.\n",
    "    os.remove (\"Plot2.pdf\")\n",
    "    os.remove (\"plot31.pdf\")\n",
    "    os.remove (\"plot32.pdf\")\n",
    "    os.remove (\"plot33.pdf\")\n",
    "    os.remove (\"plot34.pdf\")\n",
    "    os.remove (\"plot35.pdf\")\n",
    "    os.remove (\"plot36.pdf\")\n",
    "    os.remove (\"plot4.pdf\")\n",
    "    if trig_num == len(triggers) - 1 :   \t# After final trigger, close plots of geomagnetic activity\n",
    "        os.remove (\"plot_solar.pdf\")\n",
    "        if GOES_avg > -1:\n",
    "            os.remove (\"GOES_flux.pdf\")\n",
    "\n",
    "# After each trigger, close summary file, close values file\n",
    "#    if trig_num == len(triggers) - 1 :\n",
    "    summary_file.close()\n",
    "    values_file.close()\n",
    "#    print(' **** 12   Return from writing, trig_num = ', trig_num, len(triggers))\n",
    "    return\n",
    "\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main(fileid, chan_type, tmin, tmax)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze39 (fileid, chan_type, tmin, tmax):\n",
    "    # Input example\n",
    "    #fileid = '20231110'\n",
    "    #chan_type = 'PHA'\n",
    "    #ref_MET = 752966385.210472\n",
    "\n",
    "#    global tmin, tmax\n",
    "#    global summary_file, summary_file_name\n",
    "\n",
    "\n",
    "\n",
    "    # Declare global variables\n",
    "    global data_types, event_classes, max_rows, max_bins\n",
    "    global xL, Ngtx\n",
    "\n",
    "# Set up local disk directory environment\n",
    "    load_dotenv()\n",
    "    data_dir = os.getenv(\"LOCAL_DATA_DIR\")\n",
    "\n",
    "# Set up figure saving to file\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.00, 3.50] \n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# Account for format change in mid-2023\n",
    "    if int(fileid) > 20230725:\t\t\n",
    "        ph_table = get_phdata(fileid, data_dir, chan_type)\n",
    "    else:\n",
    "        ph_table = get_phdata_modified(fileid, data_dir, chan_type)\n",
    "\n",
    "################################\n",
    "    \"\"\"\n",
    "# Read in likelihood tables -- old version\n",
    "    likelihood_fileid = '20230615'\t# file name for likelihood data; this can be changed if necessary\n",
    "    input_file_name = \"ranking_\" + likelihood_fileid + \".txt\"\n",
    "\n",
    "#    global data_types, event_classes, max_rows\n",
    "\n",
    "    max_rows = 1000\t\t# Max number of rows; this can be changed if necessary\n",
    "\n",
    "    data_types = ['Latitude','Time_to_transition','Solar_angle','Gaussian_FWHM','Gaussian_Amplitude', 'Gaussian_Chi-sq','Norris_t90',\n",
    "        'Norris_Amplitude', 'Norris_Chi-sq', 'Hardness_ratio', 'GOES', 'GOES_variance', 'GOES_avg_1hr', 'GOES_var_1hr']\t\t\t\t# Data types\n",
    "\n",
    "    event_classes = ['GRB', 'Solar','Particle']\t\t\t\t# Event classes\n",
    "#    likelihood_list = [lat, transition, angle_to_Sun, FWHM, t90]\n",
    "\n",
    "#  Create 3d arrays\n",
    "#    global xL, Ngtx\n",
    "    xL =   [[ ['0' for row in range(max_rows)] for col in range(len(event_classes))] for type in range(len(data_types))]\t        #  x\n",
    "    Ngtx =   [[ ['0' for row in range(max_rows)] for col in range(len(event_classes))] for type in range(len(data_types))]\t#  N(>x)\n",
    "\n",
    "    rows = ranking (max_rows, input_file_name)\n",
    "\n",
    "    norm = [0] * len(event_classes)\t\t\t# Normalize Ngtx to 1 at row_unmber = 0\n",
    "    for itype in range (len(data_types)):\n",
    "       for events in range (len(event_classes)):\n",
    "           norm[events] =  (Ngtx [itype][events][0])\n",
    "           for row_number in range (rows [itype]):\n",
    "#                print ('$$$$$', itype, events, row_number, norm, Ngtx [itype][events][row_number] , type (Ngtx [itype][events][row_number]) )\n",
    "                Ngtx [itype][events][row_number] = Ngtx [itype][events][row_number] / norm [events]\n",
    "\n",
    "#    for itype in range (len(data_types)):\n",
    "#        print (data_types [itype],':')  \n",
    "#        for row_number in range (rows [itype]):\n",
    "#            print (row_number, end = '     ')\n",
    "#            for events in range (len(event_classes)):\t\t\n",
    "#                print (xL [itype][events][row_number], end = '  ')\n",
    "#            print ('  ',end='   ')\n",
    "#            for events in range (len(event_classes)):     \n",
    "#                print (  \"{:.3f}\".format( Ngtx [itype][events][row_number] ), end = '  ')\n",
    "#            print()\n",
    "    \"\"\"\n",
    "\n",
    "# Read in likelihood tables -- new version\n",
    "\n",
    "    # Declare local variables\n",
    "    global data_types, event_classes, midpoints, likelihood\n",
    "\n",
    "    # Input the file\n",
    "    likelihood_fileid = '20240602'\t# file name for likelihood data; this can be changed if necessary\n",
    "    input_file_name = \"ranking_\" + likelihood_fileid + \".txt\"\n",
    "\t\n",
    "    # Data types\n",
    "    data_types = ['Latitude','Time_to_transition','Solar_angle','Gaussian_FWHM','Gaussian_Amplitude', 'Gaussian_Chi-sq','Norris_t90',\n",
    "        'Norris_Amplitude', 'Norris_Chi-sq', 'Hardness_ratio', 'GOES', 'GOES_variance', 'GOES_avg_1hr', 'GOES_var_1hr']\t\t\t\t\n",
    "    \n",
    "    # Event classes\n",
    "    event_classes = ['GRB', 'Solar','Particle']\t\t\t\t\n",
    "\n",
    "    # Open the file and read the data\n",
    "    lhood_input = open ( input_file_name, 'r')\n",
    "    lines = len(lhood_input.readlines())  \t\t            # Determine length of file\n",
    "    lhood_input.seek(0)\t\t\t\t\t                    # Rewind to beginning\n",
    "    max_bins = int((lines - 1)/len(data_types))\t- 1\t        # Number of histogram bins must be the same for all data types\n",
    "\n",
    "    likelihood = [[ [0 for row in range(max_bins)] for col in range(len(event_classes))] for type in range(len(data_types))]\n",
    "    midpoints = [[ 0 for bin_num in range(max_bins)] for type in range(len(data_types))]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Read the data for each data type in data_types\n",
    "    for type in enumerate(range(len(data_types))):\n",
    "        \n",
    "        # Skip the first line\n",
    "        category = []  \n",
    "        data = lhood_input.readline().strip()\t\t\t      # Read each line and strip the contents\n",
    "\n",
    "        category = data.split()\t                              # Split the contents based on \" \"\n",
    "\n",
    "        print(category)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if category [0] != data_types[type]:            # Error if data type is incorrect\n",
    "            print ('Wrong data type: ', category[0])\n",
    "            sys.exit()\n",
    "\n",
    "        for bin in range (max_bins):\n",
    "            category = []\n",
    "            data = lhood_input.readline().strip()\t\t# Read data \t\t\t\t\n",
    "            category = data.split()\t\n",
    "            midpoints [type] [bin] = category [0]\n",
    "            likelihood [type] [0] [bin] = category [1]\n",
    "            likelihood [type] [1] [bin] = category [2]\n",
    "            likelihood [type] [2] [bin] = category [3]\n",
    "\n",
    "\n",
    "    BREAK\n",
    "\n",
    "\n",
    "    # Read the data for each data type in data_types\n",
    "    for type in range (len(data_types)):\n",
    "        category = []  \n",
    "        data = lhood_input.readline().strip()\t\t\t# Read data type\t\n",
    "\n",
    "\n",
    "        category = data.split()\t\n",
    "        if category [0] != data_types[type]:\n",
    "            print ('Wrong data type: ', category[0])\n",
    "            sys.exit()\n",
    "        for bin in range (max_bins):\n",
    "            category = []\n",
    "            data = lhood_input.readline().strip()\t\t# Read data \t\t\t\t\n",
    "            category = data.split()\t\n",
    "            midpoints [type] [bin] = category [0]\n",
    "            likelihood [type] [0] [bin] = category [1]\n",
    "            likelihood [type] [1] [bin] = category [2]\n",
    "            likelihood [type] [2] [bin] = category [3]\n",
    "\n",
    "################################\n",
    "# Read in event list with previous characterizations.\n",
    "    max_rows = 2000\n",
    "    event_file_name = \"event_classification.txt\"\n",
    "#    event_id_rows, event_UT_rows, event_type_rows = []\n",
    "    event_id_rows, event_UT_rows, event_type_rows = events_list (max_rows, event_file_name)\n",
    "\n",
    "################################\n",
    "# find MET time. How about if there are multiple files on a single day?\n",
    "    trig_index = []\n",
    "    trig_time = []\n",
    "    triggers, trig_index, trig_time = determine_MET_time (fileid, data_dir)\n",
    "#    print ('Result of finding triggers:')\n",
    "#    print (triggers)\n",
    "#    print (trig_index)\n",
    "#    print (trig_time)\n",
    "#    num = input()   \n",
    "\n",
    "################################\n",
    "# Step through triggers\n",
    "    for trig_num in range (len(trig_index)):\n",
    "        xyz = analyze_single_trigger (fileid, chan_type, data_dir, ph_table, trig_index, trig_time, trig_num, triggers,\n",
    "        event_id_rows, event_UT_rows, event_type_rows, tmin, tmax)\n",
    "    \n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
